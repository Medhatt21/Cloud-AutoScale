{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cloud Autoscaling – Direct Multi-Horizon Forecasting Pipeline\n",
        "\n",
        "## Production-Grade ML Models with Optuna Hyperparameter Tuning\n",
        "\n",
        "---\n",
        "\n",
        "### Overview\n",
        "\n",
        "This notebook implements a **production-grade direct multi-horizon forecasting pipeline** for CPU demand using real Google Cluster 2019 trace data.\n",
        "\n",
        "**Key Features:**\n",
        "- ✅ Direct multi-horizon forecasting (t+1, t+3, t+6)\n",
        "- ✅ Optuna hyperparameter optimization (40 trials per model)\n",
        "- ✅ Three separate LightGBM models for each horizon\n",
        "- ✅ No recursive forecasting - fail-fast validation\n",
        "- ✅ Train/Validation/Test split (70/15/15)\n",
        "- ✅ Integration-ready outputs for proactive autoscaling\n",
        "\n",
        "**Objective:** Train direct forecasting models for t+1, t+3, and t+6 horizons (5, 15, and 30 minutes ahead) to enable proactive autoscaling with asymmetric scaling logic.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ All imports successful\n",
            "✓ Working directory: /Users/medhatabouzeid/Documents/00-Projects/_AUS/Cloud-AutoScale/notebooks\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from lightgbm import LGBMRegressor\n",
        "import json\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "# Import project loaders\n",
        "from cloud_autoscale.data import GCP2019Loader\n",
        "\n",
        "# Configure plotting\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print('✓ All imports successful')\n",
        "print(f'✓ Working directory: {Path.cwd()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load GCP 2019 Data\n",
        "\n",
        "Loading real Google Cluster 2019 traces (no synthetic data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading GCP 2019 cluster trace data...\n",
            "======================================================================\n",
            "✓ Loaded 8,929 time steps\n",
            "✓ Time span: 0 to 44640 minutes\n",
            "✓ Duration: 744.0 hours\n",
            "\n",
            "Columns: ['step', 'time', 'cpu_demand', 'mem_demand', 'new_instances', 'new_instances_norm', 'machines_reporting']\n",
            "\n",
            "First 5 rows:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>time</th>\n",
              "      <th>cpu_demand</th>\n",
              "      <th>mem_demand</th>\n",
              "      <th>new_instances</th>\n",
              "      <th>new_instances_norm</th>\n",
              "      <th>machines_reporting</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.283436</td>\n",
              "      <td>4.215649</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2195.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>11.116977</td>\n",
              "      <td>4.543966</td>\n",
              "      <td>14451.0</td>\n",
              "      <td>9.578588</td>\n",
              "      <td>2209.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>10.353116</td>\n",
              "      <td>4.374648</td>\n",
              "      <td>15688.0</td>\n",
              "      <td>9.660715</td>\n",
              "      <td>2218.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>12.320097</td>\n",
              "      <td>4.651689</td>\n",
              "      <td>13254.0</td>\n",
              "      <td>9.492130</td>\n",
              "      <td>2221.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>12.255638</td>\n",
              "      <td>4.914910</td>\n",
              "      <td>12053.0</td>\n",
              "      <td>9.397152</td>\n",
              "      <td>2223.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   step  time  cpu_demand  mem_demand  new_instances  new_instances_norm  \\\n",
              "0     0     0   10.283436    4.215649            0.0            0.000000   \n",
              "1     1     5   11.116977    4.543966        14451.0            9.578588   \n",
              "2     2    10   10.353116    4.374648        15688.0            9.660715   \n",
              "3     3    15   12.320097    4.651689        13254.0            9.492130   \n",
              "4     4    20   12.255638    4.914910        12053.0            9.397152   \n",
              "\n",
              "   machines_reporting  \n",
              "0              2195.0  \n",
              "1              2209.0  \n",
              "2              2218.0  \n",
              "3              2221.0  \n",
              "4              2223.0  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load GCP 2019 data - FULL TRACE\n",
        "print('Loading GCP 2019 cluster trace data...')\n",
        "print('='*70)\n",
        "\n",
        "loader = GCP2019Loader(\n",
        "    processed_dir='../data/processed',\n",
        "    step_minutes=5,\n",
        "    duration_minutes=None  # Use full trace\n",
        ")\n",
        "\n",
        "df = loader.load()\n",
        "\n",
        "print(f'✓ Loaded {len(df):,} time steps')\n",
        "print(f'✓ Time span: {df[\"time\"].min():.0f} to {df[\"time\"].max():.0f} minutes')\n",
        "print(f'✓ Duration: {(df[\"time\"].max() - df[\"time\"].min()) / 60:.1f} hours')\n",
        "print(f'\\nColumns: {list(df.columns)}')\n",
        "print('\\nFirst 5 rows:')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feature Engineering (NO DATA LEAKAGE)\n",
        "\n",
        "**Critical Fix:** All rolling windows are shifted BEFORE rolling to prevent data leakage.\n",
        "\n",
        "### Features:\n",
        "1. **Lag Features** - Previous values (1, 2, 3, 6, 12 steps)\n",
        "2. **Rolling Statistics** - Moving averages (SHIFTED first)\n",
        "3. **Differencing** - Rate of change\n",
        "4. **Cyclical** - Daily patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating features (preventing data leakage)...\n",
            "======================================================================\n",
            "\n",
            "[1/4] Lag features...\n",
            "  ✓ Created 15 lag features\n",
            "\n",
            "[2/4] Rolling statistics (shifted to prevent leakage)...\n",
            "  ✓ Created 9 rolling features (no leakage)\n",
            "\n",
            "[3/4] Differencing...\n",
            "  ✓ Created 2 differencing features\n",
            "\n",
            "[4/4] Cyclical features...\n",
            "  ✓ Created 2 cyclical features\n",
            "\n",
            "[5/5] Cleaning...\n",
            "  ✓ Rows before: 8,929\n",
            "  ✓ Rows after: 8,917\n",
            "  ✓ Dropped: 12\n",
            "\n",
            "✓ Total features: 35\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Create features with NO DATA LEAKAGE\n",
        "print('Creating features (preventing data leakage)...')\n",
        "print('='*70)\n",
        "\n",
        "df_features = df.copy()\n",
        "\n",
        "# 1. Lag Features\n",
        "print('\\n[1/4] Lag features...')\n",
        "for lag in [1, 2, 3, 6, 12]:\n",
        "    df_features[f'cpu_lag{lag}'] = df_features['cpu_demand'].shift(lag)\n",
        "    df_features[f'mem_lag{lag}'] = df_features['mem_demand'].shift(lag)\n",
        "    df_features[f'evt_lag{lag}'] = df_features['new_instances_norm'].shift(lag)\n",
        "print('  ✓ Created 15 lag features')\n",
        "\n",
        "# 2. Rolling Statistics (SHIFT FIRST to prevent leakage)\n",
        "print('\\n[2/4] Rolling statistics (shifted to prevent leakage)...')\n",
        "for w in [3, 6, 12]:\n",
        "    # CRITICAL: shift(1) BEFORE rolling to prevent data leakage\n",
        "    df_features[f'cpu_ma{w}'] = df_features['cpu_demand'].shift(1).rolling(window=w, min_periods=1).mean()\n",
        "    df_features[f'mem_ma{w}'] = df_features['mem_demand'].shift(1).rolling(window=w, min_periods=1).mean()\n",
        "    df_features[f'evt_ma{w}'] = df_features['new_instances_norm'].shift(1).rolling(window=w, min_periods=1).mean()\n",
        "print('  ✓ Created 9 rolling features (no leakage)')\n",
        "\n",
        "# 3. Differencing\n",
        "print('\\n[3/4] Differencing...')\n",
        "df_features['cpu_diff1'] = df_features['cpu_demand'].diff()\n",
        "df_features['mem_diff1'] = df_features['mem_demand'].diff()\n",
        "print('  ✓ Created 2 differencing features')\n",
        "\n",
        "# 4. Cyclical time features\n",
        "print('\\n[4/4] Cyclical features...')\n",
        "df_features['sin_day'] = np.sin(2 * np.pi * df_features['step'] / 288)\n",
        "df_features['cos_day'] = np.cos(2 * np.pi * df_features['step'] / 288)\n",
        "print('  ✓ Created 2 cyclical features')\n",
        "\n",
        "# Drop NaN\n",
        "print('\\n[5/5] Cleaning...')\n",
        "rows_before = len(df_features)\n",
        "df_clean = df_features.dropna().reset_index(drop=True)\n",
        "rows_after = len(df_clean)\n",
        "\n",
        "print(f'  ✓ Rows before: {rows_before:,}')\n",
        "print(f'  ✓ Rows after: {rows_after:,}')\n",
        "print(f'  ✓ Dropped: {rows_before - rows_after:,}')\n",
        "print(f'\\n✓ Total features: {len(df_clean.columns)}')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Validation/Test Split\n",
        "\n",
        "**Split Strategy:**\n",
        "- Train: 70% (for model training)\n",
        "- Validation: 15% (for hyperparameter tuning)\n",
        "- Test: 15% (for final evaluation)\n",
        "\n",
        "**Ordered split** to preserve temporal structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAIN/VALIDATION/TEST SPLIT\n",
            "======================================================================\n",
            "\n",
            "Total samples: 8,917\n",
            "\n",
            "Train: 6,241 samples (70.0%)\n",
            "  Time: 60 - 31260 min\n",
            "\n",
            "Validation: 1,338 samples (15.0%)\n",
            "  Time: 31265 - 37950 min\n",
            "\n",
            "Test: 1,338 samples (15.0%)\n",
            "  Time: 37955 - 44640 min\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Define split indices\n",
        "total = len(df_clean)\n",
        "train_end = int(total * 0.7)\n",
        "val_end = int(total * 0.85)\n",
        "\n",
        "train = df_clean.iloc[:train_end]\n",
        "val = df_clean.iloc[train_end:val_end]\n",
        "test = df_clean.iloc[val_end:]\n",
        "\n",
        "print('='*70)\n",
        "print('TRAIN/VALIDATION/TEST SPLIT')\n",
        "print('='*70)\n",
        "print(f'\\nTotal samples: {total:,}')\n",
        "print(f'\\nTrain: {len(train):,} samples ({len(train)/total*100:.1f}%)')\n",
        "print(f'  Time: {train[\"time\"].min():.0f} - {train[\"time\"].max():.0f} min')\n",
        "print(f'\\nValidation: {len(val):,} samples ({len(val)/total*100:.1f}%)')\n",
        "print(f'  Time: {val[\"time\"].min():.0f} - {val[\"time\"].max():.0f} min')\n",
        "print(f'\\nTest: {len(test):,} samples ({len(test)/total*100:.1f}%)')\n",
        "print(f'  Time: {test[\"time\"].min():.0f} - {test[\"time\"].max():.0f} min')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating direct multi-horizon targets...\n",
            "Re-split: Train=6237, Val=1337, Test=1337\n"
          ]
        }
      ],
      "source": [
        "print(\"Creating direct multi-horizon targets...\")\n",
        "\n",
        "# Add direct horizon labels\n",
        "df_clean[\"cpu_t1\"] = df_clean[\"cpu_demand\"].shift(-1)\n",
        "df_clean[\"cpu_t3\"] = df_clean[\"cpu_demand\"].shift(-3)\n",
        "df_clean[\"cpu_t6\"] = df_clean[\"cpu_demand\"].shift(-6)\n",
        "\n",
        "# Remove rows that don't have future values\n",
        "df_h = df_clean.dropna(subset=[\"cpu_t1\",\"cpu_t3\",\"cpu_t6\"]).reset_index(drop=True)\n",
        "\n",
        "# Re-split based on horizon-safe dataset\n",
        "total = len(df_h)\n",
        "train_end = int(total * 0.7)\n",
        "val_end = int(total * 0.85)\n",
        "\n",
        "train = df_h.iloc[:train_end]\n",
        "val   = df_h.iloc[train_end:val_end]\n",
        "test  = df_h.iloc[val_end:]\n",
        "\n",
        "print(f\"Re-split: Train={len(train)}, Val={len(val)}, Test={len(test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Selection and Standardization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Targets: cpu_t1, cpu_t3, cpu_t6\n",
            "Features: 29\n",
            "\n",
            "Feature list:\n",
            "   1. new_instances_norm\n",
            "   2. cpu_lag1\n",
            "   3. mem_lag1\n",
            "   4. evt_lag1\n",
            "   5. cpu_lag2\n",
            "   6. mem_lag2\n",
            "   7. evt_lag2\n",
            "   8. cpu_lag3\n",
            "   9. mem_lag3\n",
            "  10. evt_lag3\n",
            "  11. cpu_lag6\n",
            "  12. mem_lag6\n",
            "  13. evt_lag6\n",
            "  14. cpu_lag12\n",
            "  15. mem_lag12\n",
            "  16. evt_lag12\n",
            "  17. cpu_ma3\n",
            "  18. mem_ma3\n",
            "  19. evt_ma3\n",
            "  20. cpu_ma6\n",
            "  21. mem_ma6\n",
            "  22. evt_ma6\n",
            "  23. cpu_ma12\n",
            "  24. mem_ma12\n",
            "  25. evt_ma12\n",
            "  26. cpu_diff1\n",
            "  27. mem_diff1\n",
            "  28. sin_day\n",
            "  29. cos_day\n"
          ]
        }
      ],
      "source": [
        "# Select features (exclude target and identifiers)\n",
        "drop_cols = [\n",
        "    \"step\",\"time\",\"cpu_demand\",\"mem_demand\",\"new_instances\",\n",
        "    \"machines_reporting\",\"cpu_t1\",\"cpu_t3\",\"cpu_t6\"\n",
        "]\n",
        "feature_cols = [c for c in df_h.columns if c not in drop_cols]\n",
        "\n",
        "print(f'Targets: cpu_t1, cpu_t3, cpu_t6')\n",
        "print(f'Features: {len(feature_cols)}')\n",
        "print(f'\\nFeature list:')\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f'  {i:2d}. {col}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature matrix and targets prepared for multi-horizon models.\n",
            "======================================================================\n",
            "STANDARDIZATION\n",
            "======================================================================\n",
            "\n",
            "X_train: (6237, 29)\n",
            "X_val:   (1337, 29)\n",
            "X_test:  (1337, 29)\n",
            "\n",
            "Feature stats after standardization (X_train):\n",
            "  Mean: -0.000000 (should be ~0)\n",
            "  Std:  1.000000 (should be ~1)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(train[feature_cols])\n",
        "X_val   = scaler.transform(val[feature_cols])\n",
        "X_test  = scaler.transform(test[feature_cols])\n",
        "\n",
        "y_t1_train = train[\"cpu_t1\"].values\n",
        "y_t3_train = train[\"cpu_t3\"].values\n",
        "y_t6_train = train[\"cpu_t6\"].values\n",
        "\n",
        "y_t1_val   = val[\"cpu_t1\"].values\n",
        "y_t3_val   = val[\"cpu_t3\"].values\n",
        "y_t6_val   = val[\"cpu_t6\"].values\n",
        "\n",
        "y_t1_test  = test[\"cpu_t1\"].values\n",
        "y_t3_test  = test[\"cpu_t3\"].values\n",
        "y_t6_test  = test[\"cpu_t6\"].values\n",
        "\n",
        "print(\"Feature matrix and targets prepared for multi-horizon models.\")\n",
        "print('='*70)\n",
        "print('STANDARDIZATION')\n",
        "print('='*70)\n",
        "print(f'\\nX_train: {X_train.shape}')\n",
        "print(f'X_val:   {X_val.shape}')\n",
        "print(f'X_test:  {X_test.shape}')\n",
        "print(f'\\nFeature stats after standardization (X_train):')\n",
        "print(f'  Mean: {X_train.mean():.6f} (should be ~0)')\n",
        "print(f'  Std:  {X_train.std():.6f} (should be ~1)')\n",
        "print('='*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Optimization with Optuna\n",
        "\n",
        "Using Optuna to find optimal hyperparameters for each horizon model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Optuna optimization functions defined\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "from optuna.samplers import TPESampler\n",
        "\n",
        "def objective_lgb(trial, X_train, y_train, X_val, y_val):\n",
        "    params = {\n",
        "        \"n_estimators\": trial.suggest_int(\"n_estimators\", 300, 1500),\n",
        "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.15),\n",
        "        \"max_depth\": trial.suggest_int(\"max_depth\", -1, 20),\n",
        "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 15, 200),\n",
        "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "        \"random_state\": 42,\n",
        "        \"verbose\": -1\n",
        "    }\n",
        "    model = LGBMRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_val)\n",
        "    return mean_absolute_error(y_val, pred)\n",
        "\n",
        "def tune_model(name, y_train, y_val):\n",
        "    print(f\"\\n{'='*70}\\nOPTIMIZING MODEL: {name}\\n{'='*70}\")\n",
        "    study = optuna.create_study(direction=\"minimize\", sampler=TPESampler(seed=42))\n",
        "    study.optimize(lambda trial: objective_lgb(\n",
        "        trial, X_train, y_train, X_val, y_val\n",
        "    ), n_trials=40)\n",
        "    print(f\"Best MAE ({name}): {study.best_value:.4f}\")\n",
        "    print(f\"Best Params: {study.best_params}\")\n",
        "    return study.best_params\n",
        "\n",
        "print(\"✓ Optuna optimization functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-29 14:20:17,747] A new study created in memory with name: no-name-8dba0d18-e6fc-4b8b-93d7-941890f73127\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "OPTIMIZING MODEL: t+1 model\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-29 14:20:31,670] Trial 0 finished with value: 4.237651258180617 and parameters: {'n_estimators': 749, 'learning_rate': 0.14310000289738825, 'max_depth': 15, 'num_leaves': 126, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014}. Best is trial 0 with value: 4.237651258180617.\n",
            "[I 2025-11-29 14:20:35,263] Trial 1 finished with value: 3.574452116832365 and parameters: {'n_estimators': 369, 'learning_rate': 0.1312646604084909, 'max_depth': 12, 'num_leaves': 146, 'subsample': 0.5102922471479012, 'colsample_bytree': 0.9849549260809971}. Best is trial 1 with value: 3.574452116832365.\n",
            "[I 2025-11-29 14:20:36,809] Trial 2 finished with value: 3.2670673399562893 and parameters: {'n_estimators': 1299, 'learning_rate': 0.039727475494958656, 'max_depth': 3, 'num_leaves': 49, 'subsample': 0.6521211214797689, 'colsample_bytree': 0.762378215816119}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:20:42,379] Trial 3 finished with value: 3.581114023578013 and parameters: {'n_estimators': 818, 'learning_rate': 0.05077207962772586, 'max_depth': 12, 'num_leaves': 40, 'subsample': 0.6460723242676091, 'colsample_bytree': 0.6831809216468459}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:20:43,401] Trial 4 finished with value: 3.5312138343781077 and parameters: {'n_estimators': 847, 'learning_rate': 0.1199246345950219, 'max_depth': 3, 'num_leaves': 110, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:17,876] Trial 5 finished with value: 3.414419204315663 and parameters: {'n_estimators': 1029, 'learning_rate': 0.03387337731622081, 'max_depth': 0, 'num_leaves': 191, 'subsample': 0.9828160165372797, 'colsample_bytree': 0.9041986740582306}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:25,474] Trial 6 finished with value: 3.2719724800762915 and parameters: {'n_estimators': 665, 'learning_rate': 0.023674095960893742, 'max_depth': 14, 'num_leaves': 96, 'subsample': 0.5610191174223894, 'colsample_bytree': 0.7475884550556351}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:26,074] Trial 7 finished with value: 3.4494115281830107 and parameters: {'n_estimators': 341, 'learning_rate': 0.13730485629102948, 'max_depth': 4, 'num_leaves': 138, 'subsample': 0.6558555380447055, 'colsample_bytree': 0.7600340105889054}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:42,510] Trial 8 finished with value: 3.3347475079624624 and parameters: {'n_estimators': 956, 'learning_rate': 0.03587962377357378, 'max_depth': 20, 'num_leaves': 159, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9474136752138245}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:51,811] Trial 9 finished with value: 3.975804885779123 and parameters: {'n_estimators': 1018, 'learning_rate': 0.13906239290323635, 'max_depth': 0, 'num_leaves': 51, 'subsample': 0.522613644455269, 'colsample_bytree': 0.6626651653816322}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:21:56,376] Trial 10 finished with value: 3.8419779736424213 and parameters: {'n_estimators': 1474, 'learning_rate': 0.0762658250036536, 'max_depth': 7, 'num_leaves': 18, 'subsample': 0.8029358617165022, 'colsample_bytree': 0.8451235367845726}. Best is trial 2 with value: 3.2670673399562893.\n",
            "[I 2025-11-29 14:22:12,758] Trial 11 finished with value: 3.260948280655883 and parameters: {'n_estimators': 1380, 'learning_rate': 0.011821853610161692, 'max_depth': 17, 'num_leaves': 73, 'subsample': 0.7019735064545706, 'colsample_bytree': 0.7792623653896451}. Best is trial 11 with value: 3.260948280655883.\n",
            "[I 2025-11-29 14:22:31,005] Trial 12 finished with value: 3.2764451266313266 and parameters: {'n_estimators': 1407, 'learning_rate': 0.01397300487346787, 'max_depth': 20, 'num_leaves': 75, 'subsample': 0.7106854076907608, 'colsample_bytree': 0.8247855693995731}. Best is trial 11 with value: 3.260948280655883.\n",
            "[I 2025-11-29 14:22:37,269] Trial 13 finished with value: 3.7107146388928345 and parameters: {'n_estimators': 1287, 'learning_rate': 0.05976816532601957, 'max_depth': 8, 'num_leaves': 74, 'subsample': 0.8537099938734372, 'colsample_bytree': 0.7737498323721297}. Best is trial 11 with value: 3.260948280655883.\n",
            "[I 2025-11-29 14:22:49,461] Trial 14 finished with value: 3.9240396310767434 and parameters: {'n_estimators': 1227, 'learning_rate': 0.09553555734422721, 'max_depth': 17, 'num_leaves': 57, 'subsample': 0.6928576241006011, 'colsample_bytree': 0.6759265952038472}. Best is trial 11 with value: 3.260948280655883.\n",
            "[I 2025-11-29 14:22:51,518] Trial 15 finished with value: 2.9945414310869842 and parameters: {'n_estimators': 1193, 'learning_rate': 0.010345884153697289, 'max_depth': 4, 'num_leaves': 21, 'subsample': 0.6171622493246931, 'colsample_bytree': 0.8648897167753675}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:22:54,679] Trial 16 finished with value: 3.186396340292245 and parameters: {'n_estimators': 1135, 'learning_rate': 0.01092598747663881, 'max_depth': 7, 'num_leaves': 15, 'subsample': 0.5961799392941978, 'colsample_bytree': 0.8758067124078945}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:22:57,832] Trial 17 finished with value: 3.7459292162192237 and parameters: {'n_estimators': 1144, 'learning_rate': 0.07050598359109916, 'max_depth': 6, 'num_leaves': 17, 'subsample': 0.596609562772596, 'colsample_bytree': 0.880016798258224}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:23:03,497] Trial 18 finished with value: 3.808098171170233 and parameters: {'n_estimators': 1095, 'learning_rate': 0.09965990018672741, 'max_depth': 10, 'num_leaves': 31, 'subsample': 0.5962112552596361, 'colsample_bytree': 0.9273819945727168}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:23:06,053] Trial 19 finished with value: 3.5226490624488336 and parameters: {'n_estimators': 1147, 'learning_rate': 0.04948035431435857, 'max_depth': 5, 'num_leaves': 15, 'subsample': 0.7812564285126707, 'colsample_bytree': 0.8424700195332414}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:23:10,364] Trial 20 finished with value: 3.201190129954901 and parameters: {'n_estimators': 671, 'learning_rate': 0.023715863567135148, 'max_depth': 9, 'num_leaves': 94, 'subsample': 0.8884048076501401, 'colsample_bytree': 0.9717676346407769}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:23:13,900] Trial 21 finished with value: 3.1533330744732897 and parameters: {'n_estimators': 542, 'learning_rate': 0.024643970661288768, 'max_depth': 9, 'num_leaves': 95, 'subsample': 0.8979946965703819, 'colsample_bytree': 0.9978151249740697}. Best is trial 15 with value: 2.9945414310869842.\n",
            "[I 2025-11-29 14:23:14,285] Trial 22 finished with value: 2.894784023432728 and parameters: {'n_estimators': 502, 'learning_rate': 0.022515410341141746, 'max_depth': 2, 'num_leaves': 182, 'subsample': 0.9157757529698705, 'colsample_bytree': 0.8902223292667321}. Best is trial 22 with value: 2.894784023432728.\n",
            "[I 2025-11-29 14:23:14,710] Trial 23 finished with value: 2.936892774371192 and parameters: {'n_estimators': 536, 'learning_rate': 0.02590401066867223, 'max_depth': 2, 'num_leaves': 197, 'subsample': 0.9123161127609903, 'colsample_bytree': 0.9397503979440348}. Best is trial 22 with value: 2.894784023432728.\n",
            "[I 2025-11-29 14:23:31,437] Trial 24 finished with value: 3.312805735376605 and parameters: {'n_estimators': 485, 'learning_rate': 0.04690422568505817, 'max_depth': -1, 'num_leaves': 196, 'subsample': 0.9383006162373654, 'colsample_bytree': 0.9228279570681868}. Best is trial 22 with value: 2.894784023432728.\n",
            "[I 2025-11-29 14:23:31,885] Trial 25 finished with value: 3.155546648294175 and parameters: {'n_estimators': 583, 'learning_rate': 0.06412807853445826, 'max_depth': 2, 'num_leaves': 161, 'subsample': 0.8314063332863683, 'colsample_bytree': 0.8171747762726307}. Best is trial 22 with value: 2.894784023432728.\n",
            "[I 2025-11-29 14:23:32,092] Trial 26 finished with value: 2.893382540756055 and parameters: {'n_estimators': 427, 'learning_rate': 0.029359821539977092, 'max_depth': 1, 'num_leaves': 176, 'subsample': 0.7456637855135846, 'colsample_bytree': 0.8764573297404833}. Best is trial 26 with value: 2.893382540756055.\n",
            "[I 2025-11-29 14:23:32,313] Trial 27 finished with value: 2.868401672749323 and parameters: {'n_estimators': 454, 'learning_rate': 0.09103650465840595, 'max_depth': 1, 'num_leaves': 177, 'subsample': 0.7573683904780258, 'colsample_bytree': 0.9442438105364356}. Best is trial 27 with value: 2.868401672749323.\n",
            "[I 2025-11-29 14:23:32,529] Trial 28 finished with value: 2.8381622052590214 and parameters: {'n_estimators': 443, 'learning_rate': 0.08473194219613356, 'max_depth': 1, 'num_leaves': 175, 'subsample': 0.7436235616324102, 'colsample_bytree': 0.9003015429858174}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:45,458] Trial 29 finished with value: 3.5065175103705006 and parameters: {'n_estimators': 420, 'learning_rate': 0.0969806565156927, 'max_depth': -1, 'num_leaves': 173, 'subsample': 0.7599645153563215, 'colsample_bytree': 0.9596732103983061}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:45,788] Trial 30 finished with value: 2.886894715339867 and parameters: {'n_estimators': 693, 'learning_rate': 0.08577966102460521, 'max_depth': 1, 'num_leaves': 172, 'subsample': 0.7305883778847561, 'colsample_bytree': 0.8095983210059794}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:46,112] Trial 31 finished with value: 2.8646825051141103 and parameters: {'n_estimators': 642, 'learning_rate': 0.08374785817481158, 'max_depth': 1, 'num_leaves': 170, 'subsample': 0.7439192084438944, 'colsample_bytree': 0.8179057227536363}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:46,446] Trial 32 finished with value: 2.927152965220913 and parameters: {'n_estimators': 702, 'learning_rate': 0.0864147537848388, 'max_depth': 1, 'num_leaves': 147, 'subsample': 0.7405604077350532, 'colsample_bytree': 0.7229527769991577}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:47,591] Trial 33 finished with value: 3.613416135899125 and parameters: {'n_estimators': 614, 'learning_rate': 0.11103128392844547, 'max_depth': 4, 'num_leaves': 163, 'subsample': 0.7254426108920369, 'colsample_bytree': 0.8067362169019031}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:54,841] Trial 34 finished with value: 3.603627875642715 and parameters: {'n_estimators': 310, 'learning_rate': 0.084509793104699, 'max_depth': -1, 'num_leaves': 131, 'subsample': 0.6786948106438915, 'colsample_bytree': 0.8006590613859411}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:55,184] Trial 35 finished with value: 2.9807476159326036 and parameters: {'n_estimators': 724, 'learning_rate': 0.1140170699088466, 'max_depth': 1, 'num_leaves': 150, 'subsample': 0.8357381884833983, 'colsample_bytree': 0.7183603747547331}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:56,135] Trial 36 finished with value: 3.361693695807464 and parameters: {'n_estimators': 773, 'learning_rate': 0.09089355993003038, 'max_depth': 3, 'num_leaves': 117, 'subsample': 0.7650827678815048, 'colsample_bytree': 0.9181401979104362}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:23:57,098] Trial 37 finished with value: 3.695886689197789 and parameters: {'n_estimators': 396, 'learning_rate': 0.10443819381517434, 'max_depth': 5, 'num_leaves': 182, 'subsample': 0.8056963228982277, 'colsample_bytree': 0.6323585999023397}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:24:24,392] Trial 38 finished with value: 4.344159686768048 and parameters: {'n_estimators': 832, 'learning_rate': 0.1250566589399446, 'max_depth': 0, 'num_leaves': 186, 'subsample': 0.6709290977566853, 'colsample_bytree': 0.5264558438014029}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:24:25,144] Trial 39 finished with value: 3.2637406925644683 and parameters: {'n_estimators': 618, 'learning_rate': 0.07408542008391214, 'max_depth': 3, 'num_leaves': 171, 'subsample': 0.6348014330989193, 'colsample_bytree': 0.8483687793655417}. Best is trial 28 with value: 2.8381622052590214.\n",
            "[I 2025-11-29 14:24:25,145] A new study created in memory with name: no-name-57bc7688-0080-4cd4-b66c-e791a6dcc95e\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best MAE (t+1 model): 2.8382\n",
            "Best Params: {'n_estimators': 443, 'learning_rate': 0.08473194219613356, 'max_depth': 1, 'num_leaves': 175, 'subsample': 0.7436235616324102, 'colsample_bytree': 0.9003015429858174}\n",
            "\n",
            "======================================================================\n",
            "OPTIMIZING MODEL: t+3 model\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-29 14:24:38,976] Trial 0 finished with value: 6.415426400868977 and parameters: {'n_estimators': 749, 'learning_rate': 0.14310000289738825, 'max_depth': 15, 'num_leaves': 126, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014}. Best is trial 0 with value: 6.415426400868977.\n",
            "[I 2025-11-29 14:24:43,760] Trial 1 finished with value: 6.08992920726024 and parameters: {'n_estimators': 369, 'learning_rate': 0.1312646604084909, 'max_depth': 12, 'num_leaves': 146, 'subsample': 0.5102922471479012, 'colsample_bytree': 0.9849549260809971}. Best is trial 1 with value: 6.08992920726024.\n",
            "[I 2025-11-29 14:24:45,395] Trial 2 finished with value: 5.437521138292062 and parameters: {'n_estimators': 1299, 'learning_rate': 0.039727475494958656, 'max_depth': 3, 'num_leaves': 49, 'subsample': 0.6521211214797689, 'colsample_bytree': 0.762378215816119}. Best is trial 2 with value: 5.437521138292062.\n",
            "[I 2025-11-29 14:24:51,298] Trial 3 finished with value: 5.796675528762539 and parameters: {'n_estimators': 818, 'learning_rate': 0.05077207962772586, 'max_depth': 12, 'num_leaves': 40, 'subsample': 0.6460723242676091, 'colsample_bytree': 0.6831809216468459}. Best is trial 2 with value: 5.437521138292062.\n",
            "[I 2025-11-29 14:24:52,376] Trial 4 finished with value: 6.035040650722066 and parameters: {'n_estimators': 847, 'learning_rate': 0.1199246345950219, 'max_depth': 3, 'num_leaves': 110, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 5.437521138292062.\n",
            "[I 2025-11-29 14:25:27,483] Trial 5 finished with value: 5.669656301559972 and parameters: {'n_estimators': 1029, 'learning_rate': 0.03387337731622081, 'max_depth': 0, 'num_leaves': 191, 'subsample': 0.9828160165372797, 'colsample_bytree': 0.9041986740582306}. Best is trial 2 with value: 5.437521138292062.\n",
            "[I 2025-11-29 14:25:37,664] Trial 6 finished with value: 5.431344306246776 and parameters: {'n_estimators': 665, 'learning_rate': 0.023674095960893742, 'max_depth': 14, 'num_leaves': 96, 'subsample': 0.5610191174223894, 'colsample_bytree': 0.7475884550556351}. Best is trial 6 with value: 5.431344306246776.\n",
            "[I 2025-11-29 14:25:38,355] Trial 7 finished with value: 5.867614059590983 and parameters: {'n_estimators': 341, 'learning_rate': 0.13730485629102948, 'max_depth': 4, 'num_leaves': 138, 'subsample': 0.6558555380447055, 'colsample_bytree': 0.7600340105889054}. Best is trial 6 with value: 5.431344306246776.\n",
            "[I 2025-11-29 14:26:03,072] Trial 8 finished with value: 5.683642616823416 and parameters: {'n_estimators': 956, 'learning_rate': 0.03587962377357378, 'max_depth': 20, 'num_leaves': 159, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9474136752138245}. Best is trial 6 with value: 5.431344306246776.\n",
            "[I 2025-11-29 14:26:12,507] Trial 9 finished with value: 6.3146345419560195 and parameters: {'n_estimators': 1018, 'learning_rate': 0.13906239290323635, 'max_depth': 0, 'num_leaves': 51, 'subsample': 0.522613644455269, 'colsample_bytree': 0.6626651653816322}. Best is trial 6 with value: 5.431344306246776.\n",
            "[I 2025-11-29 14:26:21,211] Trial 10 finished with value: 4.922221671778363 and parameters: {'n_estimators': 595, 'learning_rate': 0.010839605613814404, 'max_depth': 20, 'num_leaves': 79, 'subsample': 0.7999606153648697, 'colsample_bytree': 0.8451235367845726}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:26:30,075] Trial 11 finished with value: 5.052146065931895 and parameters: {'n_estimators': 565, 'learning_rate': 0.011541003520086371, 'max_depth': 19, 'num_leaves': 85, 'subsample': 0.8203936415868038, 'colsample_bytree': 0.8485779491700471}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:26:37,280] Trial 12 finished with value: 5.87466282392237 and parameters: {'n_estimators': 531, 'learning_rate': 0.07080987804589757, 'max_depth': 20, 'num_leaves': 74, 'subsample': 0.8281656598416642, 'colsample_bytree': 0.8492785746013918}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:26:39,405] Trial 13 finished with value: 5.109736609942904 and parameters: {'n_estimators': 557, 'learning_rate': 0.010919828772198052, 'max_depth': 17, 'num_leaves': 19, 'subsample': 0.8858320716284691, 'colsample_bytree': 0.8470256857987009}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:26:42,856] Trial 14 finished with value: 5.672755088510039 and parameters: {'n_estimators': 531, 'learning_rate': 0.0876198231785021, 'max_depth': 8, 'num_leaves': 78, 'subsample': 0.7390110336268644, 'colsample_bytree': 0.8336228052393175}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:05,263] Trial 15 finished with value: 5.465234193140933 and parameters: {'n_estimators': 1500, 'learning_rate': 0.010632673385234345, 'max_depth': 17, 'num_leaves': 83, 'subsample': 0.8836778881804738, 'colsample_bytree': 0.9117002165386531}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:18,065] Trial 16 finished with value: 6.03858950475545 and parameters: {'n_estimators': 643, 'learning_rate': 0.07236673688246455, 'max_depth': 18, 'num_leaves': 114, 'subsample': 0.7577724466230462, 'colsample_bytree': 0.8031547315052179}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:20,721] Trial 17 finished with value: 5.3828795355595505 and parameters: {'n_estimators': 439, 'learning_rate': 0.05464222311588026, 'max_depth': 8, 'num_leaves': 61, 'subsample': 0.8933139358849993, 'colsample_bytree': 0.8973226820560445}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:33,593] Trial 18 finished with value: 6.204540579869674 and parameters: {'n_estimators': 728, 'learning_rate': 0.09965990018672741, 'max_depth': 20, 'num_leaves': 98, 'subsample': 0.7147060060457335, 'colsample_bytree': 0.6961668080320843}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:39,113] Trial 19 finished with value: 5.648439111421652 and parameters: {'n_estimators': 1163, 'learning_rate': 0.023469615964392187, 'max_depth': 15, 'num_leaves': 25, 'subsample': 0.8302996229142289, 'colsample_bytree': 0.6212827985524578}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:43,414] Trial 20 finished with value: 5.3514586128207355 and parameters: {'n_estimators': 301, 'learning_rate': 0.05602719638137107, 'max_depth': 12, 'num_leaves': 169, 'subsample': 0.9353140005759697, 'colsample_bytree': 0.790340558283434}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:45,692] Trial 21 finished with value: 5.0202552558958535 and parameters: {'n_estimators': 542, 'learning_rate': 0.0106528522295036, 'max_depth': 17, 'num_leaves': 21, 'subsample': 0.8790461851523431, 'colsample_bytree': 0.8632096409488815}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:48,715] Trial 22 finished with value: 5.42948714619966 and parameters: {'n_estimators': 475, 'learning_rate': 0.0226534912004574, 'max_depth': 18, 'num_leaves': 33, 'subsample': 0.8033330141397101, 'colsample_bytree': 0.8730691549031339}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:27:56,286] Trial 23 finished with value: 5.239534594877648 and parameters: {'n_estimators': 624, 'learning_rate': 0.014250925706940754, 'max_depth': 18, 'num_leaves': 65, 'subsample': 0.8505991305245361, 'colsample_bytree': 0.9451188938960129}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:03,576] Trial 24 finished with value: 5.579420295292099 and parameters: {'n_estimators': 462, 'learning_rate': 0.04562409552472878, 'max_depth': 16, 'num_leaves': 90, 'subsample': 0.7726406377080326, 'colsample_bytree': 0.8178255598664895}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:05,814] Trial 25 finished with value: 5.4971374945827645 and parameters: {'n_estimators': 763, 'learning_rate': 0.02867300530829746, 'max_depth': 20, 'num_leaves': 15, 'subsample': 0.7020843771180812, 'colsample_bytree': 0.7291227589418527}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:12,310] Trial 26 finished with value: 5.358699593421726 and parameters: {'n_estimators': 584, 'learning_rate': 0.019100509844830055, 'max_depth': 14, 'num_leaves': 62, 'subsample': 0.8655673649804981, 'colsample_bytree': 0.8742002553994739}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:20,100] Trial 27 finished with value: 6.17059511162449 and parameters: {'n_estimators': 887, 'learning_rate': 0.10804029665497122, 'max_depth': 10, 'num_leaves': 120, 'subsample': 0.9377814123032908, 'colsample_bytree': 0.9889858750184128}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:23,672] Trial 28 finished with value: 6.029209129845521 and parameters: {'n_estimators': 425, 'learning_rate': 0.05966555110381888, 'max_depth': 18, 'num_leaves': 45, 'subsample': 0.9184174888511302, 'colsample_bytree': 0.9270239376762991}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:37,332] Trial 29 finished with value: 6.158386710929611 and parameters: {'n_estimators': 711, 'learning_rate': 0.14964577598733628, 'max_depth': 16, 'num_leaves': 133, 'subsample': 0.7972075695359219, 'colsample_bytree': 0.8708097545920999}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:49,230] Trial 30 finished with value: 5.778906043174467 and parameters: {'n_estimators': 771, 'learning_rate': 0.04161122796214426, 'max_depth': 14, 'num_leaves': 102, 'subsample': 0.8321031944976951, 'colsample_bytree': 0.8024062907555943}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:51,770] Trial 31 finished with value: 5.136482970614679 and parameters: {'n_estimators': 558, 'learning_rate': 0.010117944236525362, 'max_depth': 19, 'num_leaves': 23, 'subsample': 0.8983140233387944, 'colsample_bytree': 0.8411199520090454}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:53,393] Trial 32 finished with value: 5.038939012747207 and parameters: {'n_estimators': 506, 'learning_rate': 0.01819100850699777, 'max_depth': 17, 'num_leaves': 16, 'subsample': 0.8558514537819271, 'colsample_bytree': 0.85828598401036}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:55,504] Trial 33 finished with value: 5.533505490990856 and parameters: {'n_estimators': 393, 'learning_rate': 0.028063798827081408, 'max_depth': 16, 'num_leaves': 28, 'subsample': 0.8392932570416568, 'colsample_bytree': 0.9624631601940162}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:28:59,148] Trial 34 finished with value: 5.2286919469763715 and parameters: {'n_estimators': 504, 'learning_rate': 0.017474520109031152, 'max_depth': 19, 'num_leaves': 38, 'subsample': 0.7813009786814054, 'colsample_bytree': 0.7794069815643373}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:04,842] Trial 35 finished with value: 5.737513925269072 and parameters: {'n_estimators': 650, 'learning_rate': 0.03181382734549687, 'max_depth': 12, 'num_leaves': 50, 'subsample': 0.730847561732929, 'colsample_bytree': 0.8841420824281652}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:09,462] Trial 36 finished with value: 5.496220581291633 and parameters: {'n_estimators': 365, 'learning_rate': 0.04180307346078199, 'max_depth': 15, 'num_leaves': 71, 'subsample': 0.6781644354894459, 'colsample_bytree': 0.8198070392160278}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:16,415] Trial 37 finished with value: 5.259626187822275 and parameters: {'n_estimators': 834, 'learning_rate': 0.019064646890152755, 'max_depth': 10, 'num_leaves': 54, 'subsample': 0.8612204150937595, 'colsample_bytree': 0.7162643735114045}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:20,669] Trial 38 finished with value: 5.3209626887677866 and parameters: {'n_estimators': 610, 'learning_rate': 0.027461011538479456, 'max_depth': 13, 'num_leaves': 38, 'subsample': 0.8158494684839541, 'colsample_bytree': 0.5264558438014029}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:31,188] Trial 39 finished with value: 5.804422602543836 and parameters: {'n_estimators': 709, 'learning_rate': 0.03439938978402979, 'max_depth': 17, 'num_leaves': 83, 'subsample': 0.6147213965035336, 'colsample_bytree': 0.7703011337790288}. Best is trial 10 with value: 4.922221671778363.\n",
            "[I 2025-11-29 14:29:31,189] A new study created in memory with name: no-name-40f5dec5-29bb-4653-885d-7f5a507d2495\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best MAE (t+3 model): 4.9222\n",
            "Best Params: {'n_estimators': 595, 'learning_rate': 0.010839605613814404, 'max_depth': 20, 'num_leaves': 79, 'subsample': 0.7999606153648697, 'colsample_bytree': 0.8451235367845726}\n",
            "\n",
            "======================================================================\n",
            "OPTIMIZING MODEL: t+6 model\n",
            "======================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-11-29 14:29:43,585] Trial 0 finished with value: 7.676263609418591 and parameters: {'n_estimators': 749, 'learning_rate': 0.14310000289738825, 'max_depth': 15, 'num_leaves': 126, 'subsample': 0.5780093202212182, 'colsample_bytree': 0.5779972601681014}. Best is trial 0 with value: 7.676263609418591.\n",
            "[I 2025-11-29 14:29:48,307] Trial 1 finished with value: 7.018525851324915 and parameters: {'n_estimators': 369, 'learning_rate': 0.1312646604084909, 'max_depth': 12, 'num_leaves': 146, 'subsample': 0.5102922471479012, 'colsample_bytree': 0.9849549260809971}. Best is trial 1 with value: 7.018525851324915.\n",
            "[I 2025-11-29 14:29:49,942] Trial 2 finished with value: 6.062775191611534 and parameters: {'n_estimators': 1299, 'learning_rate': 0.039727475494958656, 'max_depth': 3, 'num_leaves': 49, 'subsample': 0.6521211214797689, 'colsample_bytree': 0.762378215816119}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:29:55,837] Trial 3 finished with value: 6.679342777731933 and parameters: {'n_estimators': 818, 'learning_rate': 0.05077207962772586, 'max_depth': 12, 'num_leaves': 40, 'subsample': 0.6460723242676091, 'colsample_bytree': 0.6831809216468459}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:29:56,917] Trial 4 finished with value: 6.653527935760546 and parameters: {'n_estimators': 847, 'learning_rate': 0.1199246345950219, 'max_depth': 3, 'num_leaves': 110, 'subsample': 0.7962072844310213, 'colsample_bytree': 0.5232252063599989}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:30:32,002] Trial 5 finished with value: 7.210689808089663 and parameters: {'n_estimators': 1029, 'learning_rate': 0.03387337731622081, 'max_depth': 0, 'num_leaves': 191, 'subsample': 0.9828160165372797, 'colsample_bytree': 0.9041986740582306}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:30:42,111] Trial 6 finished with value: 6.410082923408152 and parameters: {'n_estimators': 665, 'learning_rate': 0.023674095960893742, 'max_depth': 14, 'num_leaves': 96, 'subsample': 0.5610191174223894, 'colsample_bytree': 0.7475884550556351}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:30:42,807] Trial 7 finished with value: 6.6524479741952245 and parameters: {'n_estimators': 341, 'learning_rate': 0.13730485629102948, 'max_depth': 4, 'num_leaves': 138, 'subsample': 0.6558555380447055, 'colsample_bytree': 0.7600340105889054}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:31:06,023] Trial 8 finished with value: 7.253307929574188 and parameters: {'n_estimators': 956, 'learning_rate': 0.03587962377357378, 'max_depth': 20, 'num_leaves': 159, 'subsample': 0.9697494707820946, 'colsample_bytree': 0.9474136752138245}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:31:15,435] Trial 9 finished with value: 6.993139575052107 and parameters: {'n_estimators': 1018, 'learning_rate': 0.13906239290323635, 'max_depth': 0, 'num_leaves': 51, 'subsample': 0.522613644455269, 'colsample_bytree': 0.6626651653816322}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:31:20,228] Trial 10 finished with value: 6.9068220542045475 and parameters: {'n_estimators': 1474, 'learning_rate': 0.0762658250036536, 'max_depth': 7, 'num_leaves': 18, 'subsample': 0.8029358617165022, 'colsample_bytree': 0.8451235367845726}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:31:38,455] Trial 11 finished with value: 6.3503371106539275 and parameters: {'n_estimators': 1380, 'learning_rate': 0.011821853610161692, 'max_depth': 17, 'num_leaves': 73, 'subsample': 0.7019735064545706, 'colsample_bytree': 0.7792623653896451}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:31:57,739] Trial 12 finished with value: 6.562649444654452 and parameters: {'n_estimators': 1407, 'learning_rate': 0.01397300487346787, 'max_depth': 20, 'num_leaves': 75, 'subsample': 0.7106854076907608, 'colsample_bytree': 0.8247855693995731}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:32:06,334] Trial 13 finished with value: 6.724254127431501 and parameters: {'n_estimators': 1287, 'learning_rate': 0.05976816532601957, 'max_depth': 8, 'num_leaves': 74, 'subsample': 0.8537099938734372, 'colsample_bytree': 0.7737498323721297}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:32:18,946] Trial 14 finished with value: 7.2233281196544095 and parameters: {'n_estimators': 1227, 'learning_rate': 0.09553555734422721, 'max_depth': 17, 'num_leaves': 57, 'subsample': 0.6928576241006011, 'colsample_bytree': 0.6759265952038472}. Best is trial 2 with value: 6.062775191611534.\n",
            "[I 2025-11-29 14:32:21,334] Trial 15 finished with value: 5.343635227969348 and parameters: {'n_estimators': 1193, 'learning_rate': 0.010345884153697289, 'max_depth': 4, 'num_leaves': 21, 'subsample': 0.6171622493246931, 'colsample_bytree': 0.8648897167753675}. Best is trial 15 with value: 5.343635227969348.\n",
            "[I 2025-11-29 14:32:24,296] Trial 16 finished with value: 6.701619585615762 and parameters: {'n_estimators': 1176, 'learning_rate': 0.05168678863845229, 'max_depth': 5, 'num_leaves': 15, 'subsample': 0.6239535537375984, 'colsample_bytree': 0.8758067124078945}. Best is trial 15 with value: 5.343635227969348.\n",
            "[I 2025-11-29 14:32:25,159] Trial 17 finished with value: 5.963894132588848 and parameters: {'n_estimators': 1111, 'learning_rate': 0.07734576094491732, 'max_depth': 2, 'num_leaves': 36, 'subsample': 0.596609562772596, 'colsample_bytree': 0.8973226820560445}. Best is trial 15 with value: 5.343635227969348.\n",
            "[I 2025-11-29 14:32:25,672] Trial 18 finished with value: 4.921522898415798 and parameters: {'n_estimators': 1095, 'learning_rate': 0.1012479299672209, 'max_depth': 1, 'num_leaves': 31, 'subsample': 0.5964353926539934, 'colsample_bytree': 0.9273819945727168}. Best is trial 18 with value: 4.921522898415798.\n",
            "[I 2025-11-29 14:32:31,405] Trial 19 finished with value: 7.068105822419654 and parameters: {'n_estimators': 1092, 'learning_rate': 0.09952558527792289, 'max_depth': -1, 'num_leaves': 28, 'subsample': 0.7408281850813538, 'colsample_bytree': 0.9765584717144008}. Best is trial 18 with value: 4.921522898415798.\n",
            "[I 2025-11-29 14:32:34,076] Trial 20 finished with value: 6.8274226720264535 and parameters: {'n_estimators': 671, 'learning_rate': 0.10900722699928914, 'max_depth': 6, 'num_leaves': 101, 'subsample': 0.5492229999946833, 'colsample_bytree': 0.9339496624151321}. Best is trial 18 with value: 4.921522898415798.\n",
            "[I 2025-11-29 14:32:34,964] Trial 21 finished with value: 5.926844575923512 and parameters: {'n_estimators': 1145, 'learning_rate': 0.07495493839006394, 'max_depth': 2, 'num_leaves': 35, 'subsample': 0.5811797419662186, 'colsample_bytree': 0.8884220773892235}. Best is trial 18 with value: 4.921522898415798.\n",
            "[I 2025-11-29 14:32:35,508] Trial 22 finished with value: 4.920455710428752 and parameters: {'n_estimators': 1165, 'learning_rate': 0.0707318845889006, 'max_depth': 1, 'num_leaves': 28, 'subsample': 0.6065319686823168, 'colsample_bytree': 0.8425657014641553}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:43,126] Trial 23 finished with value: 6.9055796299127294 and parameters: {'n_estimators': 942, 'learning_rate': 0.09164264605984158, 'max_depth': 9, 'num_leaves': 63, 'subsample': 0.6194248190926775, 'colsample_bytree': 0.8343929274842051}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:43,707] Trial 24 finished with value: 5.003609286914649 and parameters: {'n_estimators': 1239, 'learning_rate': 0.11479125385401923, 'max_depth': 1, 'num_leaves': 18, 'subsample': 0.5405497614633179, 'colsample_bytree': 0.8192894071794806}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:44,311] Trial 25 finished with value: 5.014580149643899 and parameters: {'n_estimators': 1292, 'learning_rate': 0.11457486437600643, 'max_depth': 1, 'num_leaves': 87, 'subsample': 0.5075106058128437, 'colsample_bytree': 0.7993154342312795}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:52,206] Trial 26 finished with value: 6.966570844573423 and parameters: {'n_estimators': 1041, 'learning_rate': 0.08594904925873999, 'max_depth': -1, 'num_leaves': 41, 'subsample': 0.5459810721821354, 'colsample_bytree': 0.9385120515287854}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:55,652] Trial 27 finished with value: 7.127840517111156 and parameters: {'n_estimators': 1374, 'learning_rate': 0.12578347813485083, 'max_depth': 5, 'num_leaves': 15, 'subsample': 0.6799962509156924, 'colsample_bytree': 0.7127393313652276}. Best is trial 22 with value: 4.920455710428752.\n",
            "[I 2025-11-29 14:32:55,906] Trial 28 finished with value: 4.774604789804411 and parameters: {'n_estimators': 491, 'learning_rate': 0.105774005384173, 'max_depth': 1, 'num_leaves': 62, 'subsample': 0.8946104886087405, 'colsample_bytree': 0.8306155518993008}. Best is trial 28 with value: 4.774604789804411.\n",
            "[I 2025-11-29 14:33:01,543] Trial 29 finished with value: 6.992062458823677 and parameters: {'n_estimators': 484, 'learning_rate': 0.14964577598733628, 'max_depth': -1, 'num_leaves': 64, 'subsample': 0.8788502108245131, 'colsample_bytree': 0.6367861393932244}. Best is trial 28 with value: 4.774604789804411.\n",
            "[I 2025-11-29 14:33:08,097] Trial 30 finished with value: 6.884759817379665 and parameters: {'n_estimators': 543, 'learning_rate': 0.06647687393712604, 'max_depth': 11, 'num_leaves': 119, 'subsample': 0.9074136202529426, 'colsample_bytree': 0.7249432519869543}. Best is trial 28 with value: 4.774604789804411.\n",
            "[I 2025-11-29 14:33:08,455] Trial 31 finished with value: 4.862723615980125 and parameters: {'n_estimators': 758, 'learning_rate': 0.10588156239378231, 'max_depth': 1, 'num_leaves': 30, 'subsample': 0.9444046606765961, 'colsample_bytree': 0.80952909769274}. Best is trial 28 with value: 4.774604789804411.\n",
            "[I 2025-11-29 14:33:08,667] Trial 32 finished with value: 4.740738546066186 and parameters: {'n_estimators': 437, 'learning_rate': 0.10561244769450295, 'max_depth': 1, 'num_leaves': 43, 'subsample': 0.929993578238694, 'colsample_bytree': 0.8557725368772133}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:09,269] Trial 33 finished with value: 5.987361893877616 and parameters: {'n_estimators': 466, 'learning_rate': 0.10748774883498095, 'max_depth': 3, 'num_leaves': 53, 'subsample': 0.9380273465823781, 'colsample_bytree': 0.7999728790137364}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:09,748] Trial 34 finished with value: 5.556862431945406 and parameters: {'n_estimators': 613, 'learning_rate': 0.08669319285239672, 'max_depth': 2, 'num_leaves': 48, 'subsample': 0.9444515617849921, 'colsample_bytree': 0.8517989974683077}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:10,368] Trial 35 finished with value: 6.7561069652163726 and parameters: {'n_estimators': 306, 'learning_rate': 0.13153381702429098, 'max_depth': 4, 'num_leaves': 44, 'subsample': 0.835316790032634, 'colsample_bytree': 0.807791330360391}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:19,102] Trial 36 finished with value: 6.941345106966068 and parameters: {'n_estimators': 783, 'learning_rate': 0.10732929020695682, 'max_depth': 0, 'num_leaves': 61, 'subsample': 0.9966166383167632, 'colsample_bytree': 0.8650445075822503}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:20,754] Trial 37 finished with value: 6.874594113404243 and parameters: {'n_estimators': 428, 'learning_rate': 0.12289839834426186, 'max_depth': 6, 'num_leaves': 27, 'subsample': 0.90891049177885, 'colsample_bytree': 0.588757036236124}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:21,272] Trial 38 finished with value: 5.417148598004503 and parameters: {'n_estimators': 397, 'learning_rate': 0.06603469007209854, 'max_depth': 3, 'num_leaves': 84, 'subsample': 0.7752520170094457, 'colsample_bytree': 0.9068691727860967}. Best is trial 32 with value: 4.740738546066186.\n",
            "[I 2025-11-29 14:33:21,550] Trial 39 finished with value: 4.7559412357968895 and parameters: {'n_estimators': 579, 'learning_rate': 0.08630925790634333, 'max_depth': 1, 'num_leaves': 158, 'subsample': 0.9600619709114689, 'colsample_bytree': 0.7264488528099378}. Best is trial 32 with value: 4.740738546066186.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best MAE (t+6 model): 4.7407\n",
            "Best Params: {'n_estimators': 437, 'learning_rate': 0.10561244769450295, 'max_depth': 1, 'num_leaves': 43, 'subsample': 0.929993578238694, 'colsample_bytree': 0.8557725368772133}\n"
          ]
        }
      ],
      "source": [
        "best_t1 = tune_model(\"t+1 model\", y_t1_train, y_t1_val)\n",
        "best_t3 = tune_model(\"t+3 model\", y_t3_train, y_t3_val)\n",
        "best_t6 = tune_model(\"t+6 model\", y_t6_train, y_t6_val)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train Final Models\n",
        "\n",
        "Training final models using best tuned hyperparameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training final models using best tuned params...\n",
            "Final multi-horizon models trained.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nTraining final models using best tuned params...\")\n",
        "\n",
        "model_t1 = LGBMRegressor(**best_t1)\n",
        "model_t1.fit(X_train, y_t1_train)\n",
        "\n",
        "model_t3 = LGBMRegressor(**best_t3)\n",
        "model_t3.fit(X_train, y_t3_train)\n",
        "\n",
        "model_t6 = LGBMRegressor(**best_t6)\n",
        "model_t6.fit(X_train, y_t6_train)\n",
        "\n",
        "print(\"Final multi-horizon models trained.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Multi-Horizon Models\n",
        "\n",
        "Evaluating each horizon model on the test set.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "MULTI-HORIZON MODEL EVALUATION\n",
            "======================================================================\n",
            "horizon      MAE       R2\n",
            "    t+1 3.637210 0.885892\n",
            "    t+3 5.629380 0.765516\n",
            "    t+6 7.580705 0.526342\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "def eval_model(name, model, X, y):\n",
        "    pred = model.predict(X)\n",
        "    return {\n",
        "        \"horizon\": name,\n",
        "        \"MAE\": float(mean_absolute_error(y, pred)),\n",
        "        \"R2\": float(r2_score(y, pred))\n",
        "    }\n",
        "\n",
        "results = [\n",
        "    eval_model(\"t+1\", model_t1, X_test, y_t1_test),\n",
        "    eval_model(\"t+3\", model_t3, X_test, y_t3_test),\n",
        "    eval_model(\"t+6\", model_t6, X_test, y_t6_test)\n",
        "]\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"=\"*70)\n",
        "print(\"MULTI-HORIZON MODEL EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Save Results\n",
        "\n",
        "Saving all outputs to the simulation run directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Output directory: ../results/run_20251129_143330/modeling\n"
          ]
        }
      ],
      "source": [
        "run_dir = Path(\"../results\") / f\"run_{pd.Timestamp.now():%Y%m%d_%H%M%S}\"\n",
        "model_dir = run_dir / \"modeling\"\n",
        "model_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f'✓ Output directory: {model_dir}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved model artifacts:\n",
            " - scaler.pkl\n",
            " - model_t1.pkl\n",
            " - model_t3.pkl\n",
            " - model_t6.pkl\n",
            " - feature_cols.json\n"
          ]
        }
      ],
      "source": [
        "# Save models\n",
        "joblib.dump(model_t1, model_dir / \"model_t1.pkl\")\n",
        "joblib.dump(model_t3, model_dir / \"model_t3.pkl\")\n",
        "joblib.dump(model_t6, model_dir / \"model_t6.pkl\")\n",
        "\n",
        "# Save scaler + features\n",
        "joblib.dump(scaler, model_dir / \"scaler.pkl\")\n",
        "with open(model_dir / \"feature_cols.json\", \"w\") as f:\n",
        "    json.dump(feature_cols, f, indent=4)\n",
        "\n",
        "print(\"Saved model artifacts:\")\n",
        "for p in model_dir.iterdir():\n",
        "    print(\" -\", p.name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Summary\n",
        "\n",
        "---\n",
        "\n",
        "### Results Summary\n",
        "\n",
        "✅ **Direct Multi-Horizon Forecasting**\n",
        "- Three separate LightGBM models trained for t+1, t+3, t+6 horizons\n",
        "- Hyperparameters optimized using Optuna (40 trials per model)\n",
        "- No recursive forecasting - each model directly predicts its target horizon\n",
        "\n",
        "✅ **Production-Ready Outputs**\n",
        "- `model_t1.pkl` - Direct t+1 forecasting model\n",
        "- `model_t3.pkl` - Direct t+3 forecasting model\n",
        "- `model_t6.pkl` - Direct t+6 forecasting model\n",
        "- `scaler.pkl` - Feature standardization\n",
        "- `feature_cols.json` - Feature metadata\n",
        "\n",
        "✅ **Integration with Proactive Autoscaler**\n",
        "This version uses direct multi-horizon forecasting with tuned LightGBM models trained on t+1, t+3, t+6 horizons. The autoscaler loads all three models and uses them for asymmetric scaling decisions with error-aware safety margins.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DIRECT MULTI-HORIZON MODELING PIPELINE COMPLETE\n",
            "======================================================================\n",
            "\n",
            "📁 Output Directory: ../results/run_20251129_143330/modeling\n",
            "\n",
            "📊 Files Generated:\n",
            "  ✓ feature_cols.json                   0.5 KB\n",
            "  ✓ model_t1.pkl                      153.9 KB\n",
            "  ✓ model_t3.pkl                     4052.7 KB\n",
            "  ✓ model_t6.pkl                      151.8 KB\n",
            "  ✓ scaler.pkl                          1.8 KB\n",
            "\n",
            "🎯 Multi-Horizon Models:\n",
            "   t+1: MAE=3.6372, R²=0.8859\n",
            "   t+3: MAE=5.6294, R²=0.7655\n",
            "   t+6: MAE=7.5807, R²=0.5263\n",
            "\n",
            "✓ Ready for proactive autoscaling integration\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Final summary\n",
        "print('='*70)\n",
        "print('DIRECT MULTI-HORIZON MODELING PIPELINE COMPLETE')\n",
        "print('='*70)\n",
        "print(f'\\n📁 Output Directory: {model_dir}')\n",
        "print(f'\\n📊 Files Generated:')\n",
        "for file in sorted(model_dir.iterdir()):\n",
        "    if file.is_file():\n",
        "        size_kb = file.stat().st_size / 1024\n",
        "        print(f'  ✓ {file.name:<30} {size_kb:>8.1f} KB')\n",
        "\n",
        "print(f'\\n🎯 Multi-Horizon Models:')\n",
        "print(f'   t+1: MAE={results[0][\"MAE\"]:.4f}, R²={results[0][\"R2\"]:.4f}')\n",
        "print(f'   t+3: MAE={results[1][\"MAE\"]:.4f}, R²={results[1][\"R2\"]:.4f}')\n",
        "print(f'   t+6: MAE={results[2][\"MAE\"]:.4f}, R²={results[2][\"R2\"]:.4f}')\n",
        "print('\\n✓ Ready for proactive autoscaling integration')\n",
        "print('='*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
