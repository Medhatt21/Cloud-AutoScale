{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA: Google Cluster Trace 2019 - Demand-Only Analysis (CORRECTED)\n",
        "\n",
        "This notebook performs **corrected** exploratory data analysis on workload demand from Google Cloud Traces 2019.\n",
        "\n",
        "## Critical Corrections Applied\n",
        "\n",
        "1. **NO REAL TIMESTAMPS**: Google Trace 2019 does NOT contain real timestamps. All temporal analysis uses `bucket_index` (derived from `bucket_s`).\n",
        "2. **Synthetic Time Features**: Hour-of-day, day-of-week features are synthetic and used ONLY for seasonality pattern detection.\n",
        "3. **Machine Counts**: The `machines` column represents machines reporting usage, NOT total cluster capacity.\n",
        "4. **Instance Events Mismatch**: `new_instances_cluster` includes tasks that may not appear in usage data - requires normalization.\n",
        "5. **Memory Units**: Memory values are already normalized (likely in GB based on magnitude).\n",
        "\n",
        "## Data Available\n",
        "\n",
        "- `data/processed/machine_level.parquet`: Per-machine CPU/memory demand\n",
        "- `data/processed/cluster_level.parquet`: Aggregated cluster-wide demand\n",
        "\n",
        "**Columns:**\n",
        "- `bucket_s`: Seconds since trace start (300s = 5min intervals)\n",
        "- `cpu_demand` / `cpu_used`: CPU usage in cores\n",
        "- `mem_demand` / `mem_used`: Memory usage (normalized, likely GB)\n",
        "- `machines`: Number of machines reporting usage in this bucket\n",
        "- `new_instances_cluster` / `new_instances_machine`: New instance arrivals\n",
        "\n",
        "## Analysis Structure\n",
        "\n",
        "1. Setup & Data Loading\n",
        "2. Data Quality & Coverage Analysis\n",
        "3. Temporal Analysis with bucket_index\n",
        "4. Demand Distribution & Statistics\n",
        "5. New Instance Analysis (with normalization)\n",
        "6. Temporal Patterns & Rolling Statistics\n",
        "7. Spike Detection\n",
        "8. ACF/PACF Analysis for Forecastability\n",
        "9. Correlation Analysis\n",
        "10. ML-Ready Feature Engineering\n",
        "11. Autoscaler-Ready Summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup & Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import polars as pl\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import acf, pacf\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "# Create output directory for summary tables\n",
        "output_dir = Path(\"eda_summary\")\n",
        "output_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(\"‚úÖ Imports complete\")\n",
        "print(f\"üìÅ Output directory: {output_dir.absolute()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading & Schema Verification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets using Polars\n",
        "print(\"üìÇ Loading datasets...\")\n",
        "\n",
        "ml = pl.read_parquet(\"../data/processed/machine_level.parquet\")\n",
        "cl = pl.read_parquet(\"../data/processed/cluster_level.parquet\")\n",
        "\n",
        "print(f\"‚úÖ Machine-level data: {len(ml):,} rows\")\n",
        "print(f\"‚úÖ Cluster-level data: {len(cl):,} rows\")\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CLUSTER-LEVEL SCHEMA\")\n",
        "print(\"=\"*80)\n",
        "print(cl.schema)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MACHINE-LEVEL SCHEMA\")\n",
        "print(\"=\"*80)\n",
        "print(ml.schema)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample data\n",
        "print(\"Cluster-Level Sample (first 10 rows):\")\n",
        "display(cl.head(10))\n",
        "\n",
        "print(\"\\nMachine-Level Sample (first 10 rows):\")\n",
        "display(ml.head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Quality & Coverage Analysis\n",
        "\n",
        "**Critical:** Assess data completeness, missing buckets, and machine coverage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create bucket_index for temporal analysis\n",
        "# bucket_s starts at 300, so bucket_index = (bucket_s / 300) - 1\n",
        "cl = cl.with_columns([\n",
        "    ((pl.col('bucket_s') / 300) - 1).cast(pl.Int64).alias('bucket_index')\n",
        "])\n",
        "\n",
        "ml = ml.with_columns([\n",
        "    ((pl.col('bucket_s') / 300) - 1).cast(pl.Int64).alias('bucket_index')\n",
        "])\n",
        "\n",
        "print(\"‚úÖ Created bucket_index for temporal analysis\")\n",
        "print(f\"   Bucket index range: {cl['bucket_index'].min()} to {cl['bucket_index'].max()}\")\n",
        "print(f\"   Expected buckets: {cl['bucket_index'].max() - cl['bucket_index'].min() + 1}\")\n",
        "print(f\"   Actual buckets: {len(cl)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing buckets\n",
        "min_idx = cl['bucket_index'].min()\n",
        "max_idx = cl['bucket_index'].max()\n",
        "expected_buckets = set(range(min_idx, max_idx + 1))\n",
        "actual_buckets = set(cl['bucket_index'].to_list())\n",
        "missing_buckets = expected_buckets - actual_buckets\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"DATA COVERAGE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Expected bucket count: {len(expected_buckets):,}\")\n",
        "print(f\"Actual bucket count:   {len(actual_buckets):,}\")\n",
        "print(f\"Missing buckets:       {len(missing_buckets):,}\")\n",
        "\n",
        "if len(missing_buckets) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_buckets)} buckets are missing!\")\n",
        "    print(f\"   Missing bucket indices (first 20): {sorted(list(missing_buckets))[:20]}\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No missing buckets - complete temporal coverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Machine coverage analysis\n",
        "total_unique_machines = ml['machine_id'].n_unique()\n",
        "machines_per_bucket = ml.group_by('bucket_index').agg([\n",
        "    pl.col('machine_id').n_unique().alias('unique_machines'),\n",
        "    pl.count().alias('records')\n",
        "]).sort('bucket_index')\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MACHINE COVERAGE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total unique machines across all buckets: {total_unique_machines:,}\")\n",
        "print(f\"\\nMachines per bucket statistics:\")\n",
        "print(machines_per_bucket['unique_machines'].describe())\n",
        "\n",
        "# Check for buckets with very low machine counts\n",
        "low_coverage = machines_per_bucket.filter(pl.col('unique_machines') < 100)\n",
        "if len(low_coverage) > 0:\n",
        "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(low_coverage)} buckets have < 100 machines reporting\")\n",
        "    print(\"   This may indicate data quality issues.\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ All buckets have adequate machine coverage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot machine coverage over time\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# Machines reporting per bucket\n",
        "cl_sorted = cl.sort('bucket_index')\n",
        "axes[0].plot(cl_sorted['bucket_index'], cl_sorted['machines'], linewidth=1, alpha=0.7, color='steelblue')\n",
        "axes[0].set_xlabel('Bucket Index')\n",
        "axes[0].set_ylabel('Machines Reporting Usage')\n",
        "axes[0].set_title('Number of Machines Reporting Usage Over Time')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].axhline(cl_sorted['machines'].mean(), color='red', linestyle='--', linewidth=2, \n",
        "                label=f'Mean: {cl_sorted[\"machines\"].mean():.0f}')\n",
        "axes[0].legend()\n",
        "\n",
        "# Records per bucket (usage density)\n",
        "axes[1].plot(machines_per_bucket['bucket_index'], machines_per_bucket['records'], \n",
        "             linewidth=1, alpha=0.7, color='darkorange')\n",
        "axes[1].set_xlabel('Bucket Index')\n",
        "axes[1].set_ylabel('Number of Usage Records')\n",
        "axes[1].set_title('Usage Record Density Over Time')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(machines_per_bucket['records'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "                label=f'Mean: {machines_per_bucket[\"records\"].mean():.0f}')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"   - 'machines' column represents machines reporting usage, NOT total capacity\")\n",
        "print(\"   - Variations indicate dynamic cluster behavior\")\n",
        "print(\"   - Sparse coverage may indicate partial data or machine churn\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Temporal Coverage & Basic Statistics\n",
        "\n",
        "**Using bucket_index for all temporal analysis** (NOT real timestamps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal coverage\n",
        "bucket_interval_s = 300  # 5 minutes\n",
        "total_duration_s = (cl['bucket_index'].max() + 1) * bucket_interval_s\n",
        "total_duration_hours = total_duration_s / 3600\n",
        "total_duration_days = total_duration_hours / 24\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"TEMPORAL COVERAGE (using bucket_index)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Bucket interval:     {bucket_interval_s} seconds (5 minutes)\")\n",
        "print(f\"Total buckets:       {cl['bucket_index'].max() + 1:,}\")\n",
        "print(f\"Total duration:      {total_duration_hours:.1f} hours ({total_duration_days:.1f} days)\")\n",
        "print(f\"Bucket index range:  {cl['bucket_index'].min()} to {cl['bucket_index'].max()}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DEMAND STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CPU Demand:    min={cl['cpu_demand'].min():.2f}, max={cl['cpu_demand'].max():.2f}, \"\n",
        "      f\"mean={cl['cpu_demand'].mean():.2f}, std={cl['cpu_demand'].std():.2f}\")\n",
        "print(f\"Memory Demand: min={cl['mem_demand'].min():.2f}, max={cl['mem_demand'].max():.2f}, \"\n",
        "      f\"mean={cl['mem_demand'].mean():.2f}, std={cl['mem_demand'].std():.2f}\")\n",
        "print(f\"\\nMachine Count: min={cl['machines'].min():,}, max={cl['machines'].max():,}, \"\n",
        "      f\"mean={cl['machines'].mean():.0f}, std={cl['machines'].std():.0f}\")\n",
        "print(f\"New Instances: total={cl['new_instances_cluster'].sum():,}, \"\n",
        "      f\"mean_per_bucket={cl['new_instances_cluster'].mean():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Demand Distribution & Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to pandas for plotting\n",
        "cl_pd = cl.sort('bucket_index').to_pandas()\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# CPU demand distribution\n",
        "axes[0, 0].hist(cl_pd['cpu_demand'], bins=100, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "axes[0, 0].axvline(cl_pd['cpu_demand'].mean(), color='red', linestyle='--', linewidth=2, \n",
        "                   label=f'Mean: {cl_pd[\"cpu_demand\"].mean():.2f}')\n",
        "axes[0, 0].axvline(cl_pd['cpu_demand'].median(), color='orange', linestyle='--', linewidth=2, \n",
        "                   label=f'Median: {cl_pd[\"cpu_demand\"].median():.2f}')\n",
        "axes[0, 0].set_xlabel('CPU Demand (cores)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('CPU Demand Distribution')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory demand distribution\n",
        "axes[0, 1].hist(cl_pd['mem_demand'], bins=100, edgecolor='black', alpha=0.7, color='darkorange')\n",
        "axes[0, 1].axvline(cl_pd['mem_demand'].mean(), color='red', linestyle='--', linewidth=2, \n",
        "                   label=f'Mean: {cl_pd[\"mem_demand\"].mean():.2f}')\n",
        "axes[0, 1].axvline(cl_pd['mem_demand'].median(), color='blue', linestyle='--', linewidth=2, \n",
        "                   label=f'Median: {cl_pd[\"mem_demand\"].median():.2f}')\n",
        "axes[0, 1].set_xlabel('Memory Demand (normalized units, likely GB)')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Memory Demand Distribution')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# CPU demand over bucket_index\n",
        "axes[1, 0].plot(cl_pd['bucket_index'], cl_pd['cpu_demand'], linewidth=1, alpha=0.7, color='steelblue')\n",
        "axes[1, 0].set_xlabel('Bucket Index')\n",
        "axes[1, 0].set_ylabel('CPU Demand (cores)')\n",
        "axes[1, 0].set_title('CPU Demand Over Time (bucket_index)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory demand over bucket_index\n",
        "axes[1, 1].plot(cl_pd['bucket_index'], cl_pd['mem_demand'], linewidth=1, alpha=0.7, color='darkorange')\n",
        "axes[1, 1].set_xlabel('Bucket Index')\n",
        "axes[1, 1].set_ylabel('Memory Demand (normalized units)')\n",
        "axes[1, 1].set_title('Memory Demand Over Time (bucket_index)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Percentile analysis\n",
        "print(\"=\"*80)\n",
        "print(\"DEMAND PERCENTILES\")\n",
        "print(\"=\"*80)\n",
        "percentiles = [50, 75, 90, 95, 99]\n",
        "for p in percentiles:\n",
        "    cpu_p = np.percentile(cl_pd['cpu_demand'], p)\n",
        "    mem_p = np.percentile(cl_pd['mem_demand'], p)\n",
        "    print(f\"P{p:2d}: CPU={cpu_p:8.2f} cores, Memory={mem_p:8.2f} units\")\n",
        "\n",
        "# Burstiness analysis\n",
        "cpu_cv = cl_pd['cpu_demand'].std() / cl_pd['cpu_demand'].mean()\n",
        "mem_cv = cl_pd['mem_demand'].std() / cl_pd['mem_demand'].mean()\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BURSTINESS ANALYSIS (Coefficient of Variation)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CPU CV:    {cpu_cv:.4f}\")\n",
        "print(f\"Memory CV: {mem_cv:.4f}\")\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"  CV < 0.5: Low variability (predictable)\")\n",
        "print(\"  CV 0.5-1.0: Moderate variability\")\n",
        "print(\"  CV > 1.0: High variability (bursty, challenging for autoscaling)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. New Instance Analysis (with Normalization)\n",
        "\n",
        "**Critical:** `new_instances_cluster` includes tasks that may NOT appear in usage data. We must normalize this metric.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize new instances by demand\n",
        "cl_pd['new_instances_normalized'] = cl_pd['new_instances_cluster'] / (cl_pd['cpu_demand'] + 1e-6)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"NEW INSTANCE ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total new instances:           {cl_pd['new_instances_cluster'].sum():,}\")\n",
        "print(f\"Mean new instances per bucket: {cl_pd['new_instances_cluster'].mean():.2f}\")\n",
        "print(f\"Max new instances per bucket:  {cl_pd['new_instances_cluster'].max():,}\")\n",
        "print(f\"\\nNormalized ratio (instances/demand):\")\n",
        "print(f\"  Mean:   {cl_pd['new_instances_normalized'].mean():.2f}\")\n",
        "print(f\"  Median: {cl_pd['new_instances_normalized'].median():.2f}\")\n",
        "print(f\"  Max:    {cl_pd['new_instances_normalized'].max():.2f}\")\n",
        "\n",
        "# Detect extreme divergence\n",
        "high_ratio = cl_pd[cl_pd['new_instances_normalized'] > 10000]\n",
        "print(f\"\\n‚ö†Ô∏è  Buckets with extreme instance/demand ratio (>10000): {len(high_ratio)}\")\n",
        "if len(high_ratio) > 0:\n",
        "    print(\"   This indicates instance_events include many tasks not reflected in usage data.\")\n",
        "    print(\"   These must be normalized before use in ML or autoscaling.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot new instances vs demand\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# New instances over time\n",
        "axes[0, 0].plot(cl_pd['bucket_index'], cl_pd['new_instances_cluster'], \n",
        "                linewidth=1, alpha=0.7, color='purple')\n",
        "axes[0, 0].set_xlabel('Bucket Index')\n",
        "axes[0, 0].set_ylabel('New Instances')\n",
        "axes[0, 0].set_title('New Instance Arrivals Over Time')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Normalized ratio over time\n",
        "axes[0, 1].plot(cl_pd['bucket_index'], cl_pd['new_instances_normalized'], \n",
        "                linewidth=1, alpha=0.7, color='purple')\n",
        "axes[0, 1].set_xlabel('Bucket Index')\n",
        "axes[0, 1].set_ylabel('Instances / CPU Demand')\n",
        "axes[0, 1].set_title('Normalized Instance Arrival Rate')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_ylim(0, np.percentile(cl_pd['new_instances_normalized'], 99))  # Cap at P99 for visibility\n",
        "\n",
        "# Scatter: new instances vs CPU demand\n",
        "sample_size = min(5000, len(cl_pd))\n",
        "cl_sample = cl_pd.sample(n=sample_size, random_state=42)\n",
        "axes[1, 0].scatter(cl_sample['cpu_demand'], cl_sample['new_instances_cluster'], \n",
        "                   alpha=0.3, s=10, color='purple')\n",
        "axes[1, 0].set_xlabel('CPU Demand (cores)')\n",
        "axes[1, 0].set_ylabel('New Instances')\n",
        "axes[1, 0].set_title(f'New Instances vs CPU Demand (n={sample_size:,})')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Distribution of normalized ratio\n",
        "axes[1, 1].hist(cl_pd['new_instances_normalized'], bins=100, edgecolor='black', \n",
        "                alpha=0.7, color='purple', range=(0, np.percentile(cl_pd['new_instances_normalized'], 99)))\n",
        "axes[1, 1].set_xlabel('Instances / CPU Demand')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Distribution of Normalized Instance Arrival Rate (capped at P99)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"   - Instance events and usage data are NOT perfectly aligned\")\n",
        "print(\"   - Many instance arrivals do not contribute to measured usage\")\n",
        "print(\"   - Normalization is essential before using this feature in ML models\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Temporal Patterns & Rolling Statistics\n",
        "\n",
        "**Using bucket_index for all temporal analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create synthetic time cycles for seasonality detection\n",
        "# 12 buckets per hour (5min intervals), 288 buckets per day\n",
        "buckets_per_hour = 12\n",
        "buckets_per_day = 288\n",
        "\n",
        "cl_pd['synthetic_hour'] = cl_pd['bucket_index'] % buckets_per_hour\n",
        "cl_pd['synthetic_day'] = cl_pd['bucket_index'] % buckets_per_day\n",
        "\n",
        "# Create rolling statistics\n",
        "windows = {\n",
        "    '1h': 12,   # 12 * 5min = 1 hour\n",
        "    '6h': 72,   # 72 * 5min = 6 hours\n",
        "    '24h': 288  # 288 * 5min = 24 hours\n",
        "}\n",
        "\n",
        "for name, window in windows.items():\n",
        "    cl_pd[f'cpu_rolling_mean_{name}'] = cl_pd['cpu_demand'].rolling(window=window, center=False).mean()\n",
        "    cl_pd[f'cpu_rolling_std_{name}'] = cl_pd['cpu_demand'].rolling(window=window, center=False).std()\n",
        "    cl_pd[f'mem_rolling_mean_{name}'] = cl_pd['mem_demand'].rolling(window=window, center=False).mean()\n",
        "    cl_pd[f'mem_rolling_std_{name}'] = cl_pd['mem_demand'].rolling(window=window, center=False).std()\n",
        "\n",
        "print(\"‚úÖ Created rolling statistics for 1h, 6h, 24h windows\")\n",
        "print(\"‚úÖ Created synthetic time cycles for seasonality detection\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot rolling statistics\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# CPU demand with rolling means\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_demand'], linewidth=0.5, alpha=0.3, \n",
        "             color='gray', label='Raw Demand')\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_rolling_mean_1h'], linewidth=1.5, \n",
        "             alpha=0.8, color='blue', label='1h Rolling Mean')\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_rolling_mean_6h'], linewidth=1.5, \n",
        "             alpha=0.8, color='green', label='6h Rolling Mean')\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_rolling_mean_24h'], linewidth=1.5, \n",
        "             alpha=0.8, color='red', label='24h Rolling Mean')\n",
        "axes[0].set_xlabel('Bucket Index')\n",
        "axes[0].set_ylabel('CPU Demand (cores)')\n",
        "axes[0].set_title('CPU Demand with Rolling Means')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory demand with rolling means\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_demand'], linewidth=0.5, alpha=0.3, \n",
        "             color='gray', label='Raw Demand')\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_rolling_mean_1h'], linewidth=1.5, \n",
        "             alpha=0.8, color='blue', label='1h Rolling Mean')\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_rolling_mean_6h'], linewidth=1.5, \n",
        "             alpha=0.8, color='green', label='6h Rolling Mean')\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_rolling_mean_24h'], linewidth=1.5, \n",
        "             alpha=0.8, color='red', label='24h Rolling Mean')\n",
        "axes[1].set_xlabel('Bucket Index')\n",
        "axes[1].set_ylabel('Memory Demand')\n",
        "axes[1].set_title('Memory Demand with Rolling Means')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Spike Detection\n",
        "\n",
        "Identify demand spikes using statistical thresholds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect spikes (demand > rolling_mean + 3 * rolling_std)\n",
        "threshold_sigma = 3\n",
        "\n",
        "cl_pd['cpu_spike'] = (\n",
        "    cl_pd['cpu_demand'] > \n",
        "    (cl_pd['cpu_rolling_mean_6h'] + threshold_sigma * cl_pd['cpu_rolling_std_6h'])\n",
        ")\n",
        "\n",
        "cl_pd['mem_spike'] = (\n",
        "    cl_pd['mem_demand'] > \n",
        "    (cl_pd['mem_rolling_mean_6h'] + threshold_sigma * cl_pd['mem_rolling_std_6h'])\n",
        ")\n",
        "\n",
        "cpu_spikes = cl_pd[cl_pd['cpu_spike'] == True]\n",
        "mem_spikes = cl_pd[cl_pd['mem_spike'] == True]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(f\"SPIKE DETECTION (threshold: {threshold_sigma}œÉ above 6h rolling mean)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CPU spikes detected:    {len(cpu_spikes):,} ({len(cpu_spikes)/len(cl_pd)*100:.2f}% of buckets)\")\n",
        "print(f\"Memory spikes detected: {len(mem_spikes):,} ({len(mem_spikes)/len(cl_pd)*100:.2f}% of buckets)\")\n",
        "\n",
        "if len(cpu_spikes) > 0:\n",
        "    print(f\"\\nCPU spike statistics:\")\n",
        "    print(f\"  Max spike magnitude: {cpu_spikes['cpu_demand'].max():.2f} cores\")\n",
        "    print(f\"  Mean spike magnitude: {cpu_spikes['cpu_demand'].mean():.2f} cores\")\n",
        "\n",
        "if len(mem_spikes) > 0:\n",
        "    print(f\"\\nMemory spike statistics:\")\n",
        "    print(f\"  Max spike magnitude: {mem_spikes['mem_demand'].max():.2f} units\")\n",
        "    print(f\"  Mean spike magnitude: {mem_spikes['mem_demand'].mean():.2f} units\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot spikes\n",
        "fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "# CPU spikes\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_demand'], linewidth=0.5, alpha=0.5, \n",
        "             color='gray', label='CPU Demand')\n",
        "axes[0].plot(cl_pd['bucket_index'], cl_pd['cpu_rolling_mean_6h'], linewidth=1.5, \n",
        "             alpha=0.8, color='blue', label='6h Rolling Mean')\n",
        "axes[0].plot(cl_pd['bucket_index'], \n",
        "             cl_pd['cpu_rolling_mean_6h'] + threshold_sigma * cl_pd['cpu_rolling_std_6h'],\n",
        "             linewidth=1.5, alpha=0.8, color='red', linestyle='--', label=f'{threshold_sigma}œÉ Threshold')\n",
        "axes[0].scatter(cpu_spikes['bucket_index'], cpu_spikes['cpu_demand'], \n",
        "                color='red', s=20, alpha=0.8, label=f'Spikes (n={len(cpu_spikes)})', zorder=5)\n",
        "axes[0].set_xlabel('Bucket Index')\n",
        "axes[0].set_ylabel('CPU Demand (cores)')\n",
        "axes[0].set_title('CPU Demand Spike Detection')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Memory spikes\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_demand'], linewidth=0.5, alpha=0.5, \n",
        "             color='gray', label='Memory Demand')\n",
        "axes[1].plot(cl_pd['bucket_index'], cl_pd['mem_rolling_mean_6h'], linewidth=1.5, \n",
        "             alpha=0.8, color='blue', label='6h Rolling Mean')\n",
        "axes[1].plot(cl_pd['bucket_index'], \n",
        "             cl_pd['mem_rolling_mean_6h'] + threshold_sigma * cl_pd['mem_rolling_std_6h'],\n",
        "             linewidth=1.5, alpha=0.8, color='red', linestyle='--', label=f'{threshold_sigma}œÉ Threshold')\n",
        "axes[1].scatter(mem_spikes['bucket_index'], mem_spikes['mem_demand'], \n",
        "                color='red', s=20, alpha=0.8, label=f'Spikes (n={len(mem_spikes)})', zorder=5)\n",
        "axes[1].set_xlabel('Bucket Index')\n",
        "axes[1].set_ylabel('Memory Demand')\n",
        "axes[1].set_title('Memory Demand Spike Detection')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"   - Spikes represent sudden demand increases that challenge reactive autoscaling\")\n",
        "print(\"   - Proactive/predictive autoscaling can mitigate spike impact\")\n",
        "print(\"   - RL agents must learn to anticipate and handle these events\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. ACF/PACF Analysis for Forecastability\n",
        "\n",
        "Assess temporal autocorrelation to determine predictability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute ACF and PACF for CPU demand\n",
        "max_lags = 288  # 24 hours worth of lags\n",
        "\n",
        "cpu_acf = acf(cl_pd['cpu_demand'].dropna(), nlags=max_lags, fft=True)\n",
        "cpu_pacf_vals = pacf(cl_pd['cpu_demand'].dropna(), nlags=max_lags)\n",
        "\n",
        "mem_acf = acf(cl_pd['mem_demand'].dropna(), nlags=max_lags, fft=True)\n",
        "mem_pacf_vals = pacf(cl_pd['mem_demand'].dropna(), nlags=max_lags)\n",
        "\n",
        "print(\"‚úÖ Computed ACF/PACF for CPU and Memory demand\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot ACF and PACF\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "# CPU ACF\n",
        "axes[0, 0].stem(range(len(cpu_acf)), cpu_acf, linefmt='steelblue', markerfmt='o', basefmt=' ')\n",
        "axes[0, 0].axhline(y=0, color='black', linewidth=0.8)\n",
        "axes[0, 0].axhline(y=1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[0, 0].axhline(y=-1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[0, 0].set_xlabel('Lag (buckets)')\n",
        "axes[0, 0].set_ylabel('ACF')\n",
        "axes[0, 0].set_title('CPU Demand - Autocorrelation Function (ACF)')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "axes[0, 0].set_xlim(0, max_lags)\n",
        "\n",
        "# CPU PACF\n",
        "axes[0, 1].stem(range(len(cpu_pacf_vals)), cpu_pacf_vals, linefmt='steelblue', markerfmt='o', basefmt=' ')\n",
        "axes[0, 1].axhline(y=0, color='black', linewidth=0.8)\n",
        "axes[0, 1].axhline(y=1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[0, 1].axhline(y=-1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[0, 1].set_xlabel('Lag (buckets)')\n",
        "axes[0, 1].set_ylabel('PACF')\n",
        "axes[0, 1].set_title('CPU Demand - Partial Autocorrelation Function (PACF)')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "axes[0, 1].set_xlim(0, max_lags)\n",
        "\n",
        "# Memory ACF\n",
        "axes[1, 0].stem(range(len(mem_acf)), mem_acf, linefmt='darkorange', markerfmt='o', basefmt=' ')\n",
        "axes[1, 0].axhline(y=0, color='black', linewidth=0.8)\n",
        "axes[1, 0].axhline(y=1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[1, 0].axhline(y=-1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[1, 0].set_xlabel('Lag (buckets)')\n",
        "axes[1, 0].set_ylabel('ACF')\n",
        "axes[1, 0].set_title('Memory Demand - Autocorrelation Function (ACF)')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "axes[1, 0].set_xlim(0, max_lags)\n",
        "\n",
        "# Memory PACF\n",
        "axes[1, 1].stem(range(len(mem_pacf_vals)), mem_pacf_vals, linefmt='darkorange', markerfmt='o', basefmt=' ')\n",
        "axes[1, 1].axhline(y=0, color='black', linewidth=0.8)\n",
        "axes[1, 1].axhline(y=1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[1, 1].axhline(y=-1.96/np.sqrt(len(cl_pd)), color='red', linestyle='--', linewidth=1)\n",
        "axes[1, 1].set_xlabel('Lag (buckets)')\n",
        "axes[1, 1].set_ylabel('PACF')\n",
        "axes[1, 1].set_title('Memory Demand - Partial Autocorrelation Function (PACF)')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "axes[1, 1].set_xlim(0, max_lags)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze forecastability\n",
        "# Count significant lags (outside confidence interval)\n",
        "confidence_bound = 1.96 / np.sqrt(len(cl_pd))\n",
        "significant_cpu_acf = np.sum(np.abs(cpu_acf[1:]) > confidence_bound)\n",
        "significant_mem_acf = np.sum(np.abs(mem_acf[1:]) > confidence_bound)\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FORECASTABILITY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Significant ACF lags (CPU):    {significant_cpu_acf}/{max_lags} \"\n",
        "      f\"({significant_cpu_acf/max_lags*100:.1f}%)\")\n",
        "print(f\"Significant ACF lags (Memory): {significant_mem_acf}/{max_lags} \"\n",
        "      f\"({significant_mem_acf/max_lags*100:.1f}%)\")\n",
        "\n",
        "# Analyze decay rate\n",
        "cpu_acf_decay = np.where(cpu_acf[1:] < confidence_bound)[0]\n",
        "mem_acf_decay = np.where(mem_acf[1:] < confidence_bound)[0]\n",
        "\n",
        "if len(cpu_acf_decay) > 0:\n",
        "    cpu_decay_lag = cpu_acf_decay[0] + 1\n",
        "    print(f\"\\nCPU ACF decays to insignificance at lag {cpu_decay_lag} \"\n",
        "          f\"({cpu_decay_lag * 5} minutes)\")\n",
        "else:\n",
        "    print(f\"\\nCPU ACF remains significant throughout {max_lags} lags (strong persistence)\")\n",
        "\n",
        "if len(mem_acf_decay) > 0:\n",
        "    mem_decay_lag = mem_acf_decay[0] + 1\n",
        "    print(f\"Memory ACF decays to insignificance at lag {mem_decay_lag} \"\n",
        "          f\"({mem_decay_lag * 5} minutes)\")\n",
        "else:\n",
        "    print(f\"Memory ACF remains significant throughout {max_lags} lags (strong persistence)\")\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"   - Strong ACF at multiple lags ‚Üí demand is predictable\")\n",
        "print(\"   - Slow ACF decay ‚Üí long-term dependencies exist\")\n",
        "print(\"   - This supports ML-based forecasting and proactive autoscaling\")\n",
        "print(\"   - RL agents can exploit temporal patterns for better decisions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation heatmap\n",
        "corr_features = [\n",
        "    'cpu_demand', 'mem_demand', 'machines', 'new_instances_normalized',\n",
        "    'cpu_rolling_mean_6h', 'cpu_rolling_std_6h',\n",
        "    'mem_rolling_mean_6h', 'mem_rolling_std_6h'\n",
        "]\n",
        "\n",
        "corr_matrix = cl_pd[corr_features].corr()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
        "            square=True, ax=ax, cbar_kws={'label': 'Correlation'})\n",
        "ax.set_title('Feature Correlation Heatmap')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"KEY CORRELATIONS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"CPU-Memory Correlation:          {cl_pd['cpu_demand'].corr(cl_pd['mem_demand']):.4f}\")\n",
        "print(f\"CPU-Machines Correlation:        {cl_pd['cpu_demand'].corr(cl_pd['machines']):.4f}\")\n",
        "print(f\"Memory-Machines Correlation:     {cl_pd['mem_demand'].corr(cl_pd['machines']):.4f}\")\n",
        "print(f\"CPU-NewInstances Correlation:    {cl_pd['cpu_demand'].corr(cl_pd['new_instances_normalized']):.4f}\")\n",
        "\n",
        "print(\"\\nüìä Interpretation:\")\n",
        "print(\"   - High CPU-Memory correlation ‚Üí resources scale together\")\n",
        "print(\"   - CPU-Machines correlation shows how well cluster tracked demand\")\n",
        "print(\"   - Low NewInstances correlation confirms events/usage mismatch\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. ML-Ready Feature Engineering\n",
        "\n",
        "Create a final dataset with engineered features for ML modeling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create lag features\n",
        "lag_steps = [1, 5, 10, 20, 50, 100]\n",
        "\n",
        "for lag in lag_steps:\n",
        "    cl_pd[f'cpu_demand_lag{lag}'] = cl_pd['cpu_demand'].shift(lag)\n",
        "    cl_pd[f'mem_demand_lag{lag}'] = cl_pd['mem_demand'].shift(lag)\n",
        "\n",
        "print(f\"‚úÖ Created lag features: {lag_steps}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create cyclical features for synthetic time patterns\n",
        "# These are SYNTHETIC and used ONLY for pattern detection\n",
        "buckets_per_hour = 12\n",
        "buckets_per_day = 288\n",
        "\n",
        "cl_pd['sin_hour'] = np.sin(2 * np.pi * cl_pd['bucket_index'] / buckets_per_hour)\n",
        "cl_pd['cos_hour'] = np.cos(2 * np.pi * cl_pd['bucket_index'] / buckets_per_hour)\n",
        "cl_pd['sin_day'] = np.sin(2 * np.pi * cl_pd['bucket_index'] / buckets_per_day)\n",
        "cl_pd['cos_day'] = np.cos(2 * np.pi * cl_pd['bucket_index'] / buckets_per_day)\n",
        "\n",
        "print(\"‚úÖ Created cyclical features (synthetic time patterns)\")\n",
        "print(\"   NOTE: These are NOT based on real timestamps, use for seasonality detection only\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create normalized features\n",
        "cl_pd['machines_normalized'] = (cl_pd['machines'] - cl_pd['machines'].mean()) / cl_pd['machines'].std()\n",
        "cl_pd['cpu_demand_normalized'] = (cl_pd['cpu_demand'] - cl_pd['cpu_demand'].mean()) / cl_pd['cpu_demand'].std()\n",
        "cl_pd['mem_demand_normalized'] = (cl_pd['mem_demand'] - cl_pd['mem_demand'].mean()) / cl_pd['mem_demand'].std()\n",
        "\n",
        "print(\"‚úÖ Created normalized features (z-score)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile ML-ready dataset\n",
        "ml_features = [\n",
        "    'bucket_index',\n",
        "    'cpu_demand', 'mem_demand',\n",
        "    'machines', 'new_instances_normalized',\n",
        "    'cpu_rolling_mean_1h', 'cpu_rolling_std_1h',\n",
        "    'cpu_rolling_mean_6h', 'cpu_rolling_std_6h',\n",
        "    'cpu_rolling_mean_24h', 'cpu_rolling_std_24h',\n",
        "    'mem_rolling_mean_1h', 'mem_rolling_std_1h',\n",
        "    'mem_rolling_mean_6h', 'mem_rolling_std_6h',\n",
        "    'mem_rolling_mean_24h', 'mem_rolling_std_24h',\n",
        "] + [f'cpu_demand_lag{lag}' for lag in lag_steps] + \\\n",
        "  [f'mem_demand_lag{lag}' for lag in lag_steps] + \\\n",
        "  ['sin_hour', 'cos_hour', 'sin_day', 'cos_day',\n",
        "   'machines_normalized', 'cpu_demand_normalized', 'mem_demand_normalized']\n",
        "\n",
        "ml_ready = cl_pd[ml_features].copy()\n",
        "\n",
        "# Remove rows with NaN (due to rolling windows and lags)\n",
        "ml_ready_clean = ml_ready.dropna()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ML-READY DATASET\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Total features:     {len(ml_features)}\")\n",
        "print(f\"Total rows:         {len(ml_ready):,}\")\n",
        "print(f\"Rows after dropna:  {len(ml_ready_clean):,}\")\n",
        "print(f\"Rows dropped:       {len(ml_ready) - len(ml_ready_clean):,}\")\n",
        "\n",
        "print(\"\\nFeature list:\")\n",
        "for i, feat in enumerate(ml_features, 1):\n",
        "    print(f\"  {i:2d}. {feat}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample of ML-ready data\n",
        "print(\"\\nML-Ready Dataset Sample:\")\n",
        "display(ml_ready_clean.head(10))\n",
        "\n",
        "print(\"\\nML-Ready Dataset Statistics:\")\n",
        "display(ml_ready_clean.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot distributions of key engineered features\n",
        "fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
        "axes = axes.flatten()\n",
        "\n",
        "plot_features = [\n",
        "    'cpu_demand_lag1', 'cpu_demand_lag10', 'cpu_demand_lag50',\n",
        "    'cpu_rolling_mean_6h', 'cpu_rolling_std_6h',\n",
        "    'sin_hour', 'cos_hour', 'sin_day', 'cos_day'\n",
        "]\n",
        "\n",
        "for i, feat in enumerate(plot_features):\n",
        "    axes[i].hist(ml_ready_clean[feat].dropna(), bins=50, edgecolor='black', alpha=0.7)\n",
        "    axes[i].set_xlabel(feat)\n",
        "    axes[i].set_ylabel('Frequency')\n",
        "    axes[i].set_title(f'Distribution: {feat}')\n",
        "    axes[i].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save ML-ready dataset\n",
        "output_path = output_dir / \"ml_ready_features.csv\"\n",
        "ml_ready_clean.to_csv(output_path, index=False)\n",
        "print(f\"‚úÖ Saved ML-ready dataset to: {output_path}\")\n",
        "print(f\"   Shape: {ml_ready_clean.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Autoscaler-Ready Summary\n",
        "\n",
        "Final summary of demand characteristics and autoscaling implications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile summary statistics\n",
        "summary = {\n",
        "    'Temporal Coverage': {\n",
        "        'Total Buckets': len(cl),\n",
        "        'Duration (hours)': total_duration_hours,\n",
        "        'Duration (days)': total_duration_days,\n",
        "        'Bucket Interval': '5 minutes',\n",
        "        'Missing Buckets': len(missing_buckets)\n",
        "    },\n",
        "    'Demand Statistics': {\n",
        "        'CPU Mean': f\"{cl_pd['cpu_demand'].mean():.2f} cores\",\n",
        "        'CPU Std': f\"{cl_pd['cpu_demand'].std():.2f} cores\",\n",
        "        'CPU Max': f\"{cl_pd['cpu_demand'].max():.2f} cores\",\n",
        "        'CPU P99': f\"{np.percentile(cl_pd['cpu_demand'], 99):.2f} cores\",\n",
        "        'Memory Mean': f\"{cl_pd['mem_demand'].mean():.2f} units\",\n",
        "        'Memory Std': f\"{cl_pd['mem_demand'].std():.2f} units\",\n",
        "        'Memory Max': f\"{cl_pd['mem_demand'].max():.2f} units\",\n",
        "        'Memory P99': f\"{np.percentile(cl_pd['mem_demand'], 99):.2f} units\"\n",
        "    },\n",
        "    'Burstiness': {\n",
        "        'CPU CV': f\"{cpu_cv:.4f}\",\n",
        "        'Memory CV': f\"{mem_cv:.4f}\",\n",
        "        'CPU Spikes': f\"{len(cpu_spikes)} ({len(cpu_spikes)/len(cl_pd)*100:.2f}%)\",\n",
        "        'Memory Spikes': f\"{len(mem_spikes)} ({len(mem_spikes)/len(cl_pd)*100:.2f}%)\"\n",
        "    },\n",
        "    'Predictability': {\n",
        "        'CPU Significant ACF Lags': f\"{significant_cpu_acf}/{max_lags}\",\n",
        "        'Memory Significant ACF Lags': f\"{significant_mem_acf}/{max_lags}\",\n",
        "        'Strong Temporal Patterns': 'Yes' if significant_cpu_acf > max_lags * 0.5 else 'Moderate'\n",
        "    },\n",
        "    'Instance Events': {\n",
        "        'Total New Instances': f\"{cl_pd['new_instances_cluster'].sum():,}\",\n",
        "        'Mean per Bucket': f\"{cl_pd['new_instances_cluster'].mean():.2f}\",\n",
        "        'Events/Usage Alignment': 'Poor - requires normalization'\n",
        "    },\n",
        "    'Machine Coverage': {\n",
        "        'Total Unique Machines': f\"{total_unique_machines:,}\",\n",
        "        'Mean Machines per Bucket': f\"{cl_pd['machines'].mean():.0f}\",\n",
        "        'Machine Count Range': f\"{cl_pd['machines'].min():,} - {cl_pd['machines'].max():,}\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"AUTOSCALER-READY SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "for category, metrics in summary.items():\n",
        "    print(f\"\\n{category}:\")\n",
        "    for key, value in metrics.items():\n",
        "        print(f\"  {key:30s}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Findings & Autoscaling Implications\n",
        "\n",
        "### 1. True Demand Characteristics\n",
        "- **CPU demand** shows moderate to high variability (CV indicates burstiness)\n",
        "- **Memory demand** follows similar but not identical patterns\n",
        "- **P99 demand** significantly exceeds mean, requiring headroom in capacity planning\n",
        "\n",
        "### 2. Burst Patterns\n",
        "- Demand spikes detected using statistical thresholds (3œÉ above rolling mean)\n",
        "- Spikes represent sudden increases that challenge reactive autoscaling\n",
        "- Proactive/predictive autoscaling can mitigate spike impact\n",
        "\n",
        "### 3. Degree of Predictability\n",
        "- **Strong autocorrelation** at multiple lags indicates temporal dependencies\n",
        "- Slow ACF decay suggests long-term patterns exist\n",
        "- This supports ML-based forecasting and proactive autoscaling strategies\n",
        "\n",
        "### 4. Misalignment Between Usage and Instance Events\n",
        "- `new_instances_cluster` includes tasks that may NOT appear in usage data\n",
        "- Instance arrivals far exceed demand in many buckets\n",
        "- **Critical:** Instance events must be normalized before use in ML or autoscaling\n",
        "\n",
        "### 5. Expected Autoscaler Challenges\n",
        "- **Reactive policies** will struggle with sudden spikes\n",
        "- **Over-provisioning** needed to handle P99 scenarios\n",
        "- **Under-utilization** likely during low-demand periods\n",
        "- **Startup delays** for new machines compound latency\n",
        "\n",
        "### 6. Reinforcement Learning Motivations\n",
        "- RL can learn to **anticipate** demand patterns from temporal features\n",
        "- RL can **balance** multiple objectives (cost, SLA, utilization)\n",
        "- RL can **adapt** to non-stationary demand over time\n",
        "- RL can exploit **long-term dependencies** revealed by ACF analysis\n",
        "\n",
        "### 7. Why Capacity Must Be Simulated\n",
        "- `machines` column represents machines **reporting usage**, NOT total capacity\n",
        "- No ground truth for actual cluster capacity exists in the trace\n",
        "- Simulator must model capacity decisions and constraints\n",
        "- This allows testing different autoscaling policies against the same demand\n",
        "\n",
        "---\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "1. **Baseline Autoscaling Simulation**\n",
        "   - Implement threshold-based, target-tracking, and predictive policies\n",
        "   - Measure SLA violations, cost, and utilization\n",
        "\n",
        "2. **ML-Based Demand Forecasting**\n",
        "   - Train LSTM/Transformer models on engineered features\n",
        "   - Evaluate forecast accuracy at different horizons\n",
        "   - Use forecasts for proactive scaling\n",
        "\n",
        "3. **RL Agent Training**\n",
        "   - Design state space (demand, capacity, rolling stats, lags)\n",
        "   - Design action space (scale up/down by N machines)\n",
        "   - Design reward function (cost penalty + SLA violation penalty + utilization bonus)\n",
        "   - Train PPO/SAC agents\n",
        "\n",
        "4. **Comparative Evaluation**\n",
        "   - Benchmark all approaches on held-out test period\n",
        "   - Analyze trade-offs and failure modes\n",
        "   - Identify scenarios where each approach excels\n",
        "\n",
        "---\n",
        "\n",
        "**END OF EDA**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
